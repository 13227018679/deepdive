{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDive Tutorial <small>Extracting mentions of spouses from the news</small>\n",
    "\n",
    "In this tutorial, we show an example of a prototypical task that DeepDive is often applied to:\n",
    "extraction of _structured information_ from _unstructured or 'dark' data_ such as web pages, text documents, images, etc.\n",
    "While DeepDive can be used as a more general platform for statistical learning and data processing, most of the tooling described herein has been built for this type of use case, based on our experience of successfully applying DeepDive to [a variety of real-world problems of this type](http://deepdive.stanford.edu/showcase/apps).\n",
    "\n",
    "In this setting, our goal is to take in a set of unstructured (and/or structured) inputs, and populate a relational database table with extracted outputs, along with marginal probabilities for each extraction representing DeepDive's confidence in the extraction.\n",
    "More formally, we write a DeepDive application to extract mentions of _relations_ and their constituent _entities_ or _attributes_, according to a specified schema; this task is often referred to as **_relation extraction_**.*\n",
    "Accordingly, we'll walk through an example scenario where we wish to extract mentions of two people being spouses from news articles.\n",
    "\n",
    "The high-level steps we'll follow are:\n",
    "\n",
    "1. **Data processing.** First, we'll load the raw corpus, add NLP markups, extract a set of _candidate_ relation mentions, and a sparse _feature_ representation of each.\n",
    "\n",
    "2. **Distant supervision with data and rules.** Next, we'll use various strategies to provide _supervision_ for our dataset, so that we can use machine learning to learn the weights of a model.\n",
    "\n",
    "3. **Learning and inference: model specification.** Then, we'll specify the high-level configuration of our _model_.\n",
    "\n",
    "4. **Error analysis and debugging.** Finally, we'll show how to use DeepDive's labeling, error analysis and debugging tools.\n",
    "\n",
    "*_Note the distinction between extraction of true, i.e., factual, relations and extraction of mentions of relations.\n",
    "In this tutorial, we do the latter, however DeepDive supports further downstream methods for tackling the former task in a principled manner._\n",
    "\n",
    "\n",
    "Whenever something isn't clear, you can always refer to [the complete example code at `examples/spouse/`](https://github.com/HazyResearch/deepdive/tree/master/examples/spouse/) that contains everything shown in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "\n",
    "### 0.0. Installing DeepDive and tweaking notebook\n",
    "First of all, let's make sure DeepDive is installed and can be used from this notebook.\n",
    "See [DeepDive installation guide](http://deepdive.stanford.edu/installation) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/ConfinedWater/local/bin:/ConfinedWater/venv/bin:/opt/homebrew/opt/postgresql/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin:/Library/TeX/texbin\n",
      "env: PATH=/ConfinedWater/spouse/deepdive/bin:/ConfinedWater/venv/bin:/opt/homebrew/opt/postgresql/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin:/Library/TeX/texbin\n",
      "deepdive is /ConfinedWater/spouse/deepdive/bin/deepdive\r\n"
     ]
    }
   ],
   "source": [
    "# PATH needs correct setup to use DeepDive\n",
    "import os; PWD=os.getcwd(); HOME=os.environ[\"HOME\"]; PATH=os.environ[\"PATH\"]\n",
    "# home directory installation\n",
    "%env PATH=$HOME/local/bin:$PATH\n",
    "# notebook-local installation\n",
    "%env PATH=$PWD/deepdive/bin:$PATH\n",
    "\n",
    "!type deepdive\n",
    "no_deepdive_found = !type deepdive >/dev/null\n",
    "if no_deepdive_found: # install it next to this notebook\n",
    "    !bash -c 'PREFIX=\"$PWD\"/deepdive bash <(curl -fsSL git.io/getdeepdive) deepdive_from_release'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tweak a few more things in this IPython/Jupyter notebook to make DeepDive work nice with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEEPDIVE_INTERACTIVE=false\n",
      "env: DEEPDIVE_PROGRESS_FD=false\n"
     ]
    }
   ],
   "source": [
    "%env DEEPDIVE_INTERACTIVE=false\n",
    "%env DEEPDIVE_PROGRESS_FD=false\n",
    "%mkdir -p input udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Declaring what to predict\n",
    "\n",
    "Above all, we shall tell DeepDive what we want to predict as a *random variable* in a language called *DDlog*, stored in a file `app.ddlog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file app.ddlog\n",
    "## Random variable to predict #################################################\n",
    "\n",
    "# This application's goal is to predict whether a given pair of person mention\n",
    "# are indicating a spouse relationship or not.\n",
    "has_spouse?(\n",
    "    p1_id text,\n",
    "    p2_id text\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to write our application in this `app.ddlog` one part at a time.\n",
    "We can check if the code make sense by asking DeepDive to compile it.\n",
    "DeepDive automatically compiles our application whenever we execute things after making changes, but we can also do this manually by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-08 03:41:08.080429 ���run/LATEST.COMPILE��� -> ���20160708/034108.038558000���\n",
      "2016-07-08 03:41:08.080742 ���run/RUNNING.COMPILE��� -> ���20160708/034108.038558000���\n",
      "2016-07-08 03:41:08.080765 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 03:41:08.080778  run/compiled/deepdive.conf\n",
      "2016-07-08 03:41:08.080788   from app.ddlog\n",
      "2016-07-08 03:41:08.422024  run/compiled/deepdive.conf.json\n",
      "2016-07-08 03:41:08.528318 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 03:41:08.555920  checking if input_extractors_well_defined\n",
      "2016-07-08 03:41:08.555970  checking if input_schema_wellformed\n",
      "2016-07-08 03:41:08.556261 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 03:41:08.556653  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 03:41:08.570965  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 03:41:08.583424  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 03:41:08.600810  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 03:41:08.613624  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 03:41:08.627775  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 03:41:08.643555  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 03:41:08.665463  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 03:41:08.753156  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 03:41:08.790837  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 03:41:08.805161  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 03:41:08.817863  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 03:41:08.831118  run/compiled/config.json\n",
      "2016-07-08 03:41:08.834070 Validating run/compiled/config.json:\n",
      "2016-07-08 03:41:08.865386  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 03:41:08.865442  checking if compiled_dependencies_correct\n",
      "2016-07-08 03:41:08.865464  checking if compiled_input_output_well_defined\n",
      "2016-07-08 03:41:08.865484  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 03:41:08.865716 Compiling executable code into:\n",
      "2016-07-08 03:41:08.866047  run/compiled/code-Makefile.json\n",
      "2016-07-08 03:41:08.866437  run/compiled/code-cmd_extractor.json\n",
      "2016-07-08 03:41:08.866788  run/compiled/code-dataflow_dot.json\n",
      "2016-07-08 03:41:08.867207  run/compiled/code-sql_extractor.json\n",
      "2016-07-08 03:41:08.867697  run/compiled/code-tsX_extractor.json\n",
      "2016-07-08 03:41:08.901812 Validating run/compiled/code-*.json:\n",
      "2016-07-08 03:41:08.928180  checking if codegen_no_path_collision\n",
      "2016-07-08 03:41:08.932593 Generating files:\n",
      "2016-07-08 03:41:08.950142  run/Makefile\n",
      "2016-07-08 03:41:08.950514  run/process/init/app/run.sh\n",
      "2016-07-08 03:41:08.950896  run/process/init/relation/has_spouse/run.sh\n",
      "2016-07-08 03:41:08.951281  run/dataflow.dot\n",
      "2016-07-08 03:41:08.995256  run/dataflow.pdf\n",
      "2016-07-08 03:41:08.996293   (file:///ConfinedWater/spouse/run/dataflow.pdf)\n",
      "2016-07-08 03:41:09.018105  run/dataflow.svg\n",
      "2016-07-08 03:41:09.018932   (file:///ConfinedWater/spouse/run/dataflow.svg)\n",
      "2016-07-08 03:41:09.247790 ���run/compiled��� -> ���run/compiled~���\n",
      "2016-07-08 03:41:09.250200 ���run/compiled��� -> ���20160708/034108.038558000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Setting up a database\n",
    "\n",
    "Next, DeepDive will store all data—input, intermediate, output, etc.—in a relational database.\n",
    "Currently, Postgres and Greenplum are supported.\n",
    "For operating at a larger scale, Greenplum is strongly recommended.\n",
    "To set the location of this database, we need to configure a URL in the [`db.url` file](../examples/spouse/db.url), e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo 'postgresql://localhost/deepdive_spouse_$USER' >db.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: DeepDive will drop and then create this database if run from scratch—beware of pointing to an existing populated one!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "���process/init/app.done��� -> ���process/init/app.done~���\n",
      "���run/RUNNING��� -> ���20160708/034109.784933000���\n",
      "���run/LATEST��� -> ���20160708/034109.784933000���\n",
      "2016-07-08 03:41:09.931761 #!/bin/sh\n",
      "2016-07-08 03:41:09.931932 # Host: scuba.local\n",
      "2016-07-08 03:41:09.931950 # App: /ConfinedWater/spouse\n",
      "2016-07-08 03:41:09.931961 # Targets: init/app\n",
      "2016-07-08 03:41:09.931972 # Plan: run/20160708/034109.784933000/plan.sh\n",
      "2016-07-08 03:41:09.931983 export DEEPDIVE_PWD='/ConfinedWater/spouse'\n",
      "2016-07-08 03:41:09.931993 # execution plan for process/init/app\n",
      "2016-07-08 03:41:09.932003 \n",
      "2016-07-08 03:41:09.932014 ## process/init/app ##########################################################\n",
      "2016-07-08 03:41:09.932025 # Done: 2016-07-08T01:49:25-0700 (1h 51m 44s ago)\n",
      "2016-07-08 03:41:09.932035 process/init/app/run.sh\n",
      "2016-07-08 03:41:09.932045 ++ dirname process/init/app/run.sh\n",
      "2016-07-08 03:41:09.932056 + cd process/init/app\n",
      "2016-07-08 03:41:09.932067 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/init/app\n",
      "2016-07-08 03:41:09.932077 + DEEPDIVE_CURRENT_PROCESS_NAME=process/init/app\n",
      "2016-07-08 03:41:09.932087 + cd /ConfinedWater/spouse\n",
      "2016-07-08 03:41:09.932098 + [[ -x init ]]\n",
      "2016-07-08 03:41:09.932108 + deepdive db init\n",
      "2016-07-08 03:41:10.465310 + [[ -r schema.sql ]]\n",
      "2016-07-08 03:41:10.465381 + [[ -x input/init.sh ]]\n",
      "2016-07-08 03:41:10.465735 mark_done process/init/app\n",
      "2016-07-08 03:41:10.483049 ##############################################################################\n",
      "2016-07-08 03:41:10.483101 \n",
      "���run/FINISHED��� -> ���run/FINISHED~���\n",
      "���run/FINISHED��� -> ���20160708/034109.784933000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo init/app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data processing\n",
    "\n",
    "In this section, we'll generate the traditional inputs of a statistical learning-type problem: candidate spouse relations, represented by a set of features, which we will aim to classify as _actual_ relation mentions or not.\n",
    "\n",
    "We'll do this in four basic steps:\n",
    "\n",
    "1. Loading raw input data\n",
    "2. Adding NLP markups\n",
    "3. Extracting candidate relation mentions\n",
    "4. Extracting features for each candidate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading raw input data\n",
    "Our first task is to download and load the raw text of [a corpus of news articles provided by Signal Media](http://research.signalmedia.co/newsir16/signal-dataset.html) into an `articles` table in our database.\n",
    "\n",
    "Keeping the identifier of each article and its content in the table would be good enough.\n",
    "We can tell DeepDive to do this by declaring the schema of this `articles` table in our `app.ddlog` file; we add the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "## Input Data #################################################################\n",
    "articles(\n",
    "    id      text,\n",
    "    content text\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepDive can use a script's output as a data source for loading data into the table if we follow a simple naming convention.\n",
    "We create a simple shell script at `input/articles.tsj.sh` that outputs the news articles in TSJ format (tab-separated JSONs) from the downloaded corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input/articles.tsj.sh\n"
     ]
    }
   ],
   "source": [
    "%%file input/articles.tsj.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "cd \"$(dirname \"$0\")\"\n",
    "\n",
    "corpus=signalmedia/signalmedia-1m.jsonl\n",
    "[[ -e \"$corpus\" ]] || {\n",
    "    echo \"ERROR: Missing $PWD/$corpus\"\n",
    "    echo \"# Please Download it from http://research.signalmedia.co/newsir16/signal-dataset.html\"\n",
    "    echo\n",
    "    echo \"# Alternatively, use our sampled data by running:\"\n",
    "    echo \"deepdive load articles input/articles-100.tsv.bz2\"\n",
    "    echo\n",
    "    echo \"# Or, skipping all NLP markup processes by running:\"\n",
    "    echo \"deepdive create table sentences\"\n",
    "    echo \"deepdive load sentences\"\n",
    "    echo \"deepdive mark done sentences\"\n",
    "    false\n",
    "} >&2\n",
    "\n",
    "cat \"$corpus\" |\n",
    "#grep -E 'wife|husband|married' |\n",
    "#head -100 |\n",
    "jq -r '[.id, .content] | map(@json) | join(\"\\t\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to mark the script as an executable so DeepDive can actually execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x input/articles.tsj.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned script reads a sample of the corpus (provided as lines of JSON objects), and then using the [jq](https://stedolan.github.io/jq/) language extracts the fields `id` (for article identifier) and `content` from each entry and format those into TSJ.\n",
    "We can uncomment the `grep` or `head` lines in between and apply some naive filter to subsample articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we tell DeepDive to execute the steps to load the `articles` table using the `input/articles.tsj.sh` script.\n",
    "You must have the [full corpus](http://research.signalmedia.co/newsir16/signal-dataset.html) downloaded at `input/signalmedia/signalmedia-1m.jsonl` for the following to finish correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mapp.ddlog: updated since last `deepdive compile`\n",
      "\u001b[0m2016-07-08 03:41:10.909955 ���run/LATEST.COMPILE��� -> ���20160708/034110.869722000���\n",
      "2016-07-08 03:41:10.910133 ���run/RUNNING.COMPILE��� -> ���20160708/034110.869722000���\n",
      "2016-07-08 03:41:10.910150 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 03:41:10.910162  run/compiled/deepdive.conf\n",
      "2016-07-08 03:41:10.910173   from app.ddlog\n",
      "2016-07-08 03:41:11.265532  run/compiled/deepdive.conf.json\n",
      "2016-07-08 03:41:11.360234 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 03:41:11.387367  checking if input_extractors_well_defined\n",
      "2016-07-08 03:41:11.387424  checking if input_schema_wellformed\n",
      "2016-07-08 03:41:11.387763 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 03:41:11.388185  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 03:41:11.401606  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 03:41:11.414268  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 03:41:11.432619  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 03:41:11.445114  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 03:41:11.459332  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 03:41:11.473823  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 03:41:11.496765  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 03:41:11.571105  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 03:41:11.603644  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 03:41:11.617249  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 03:41:11.631069  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 03:41:11.645185  run/compiled/config.json\n",
      "2016-07-08 03:41:11.647748 Validating run/compiled/config.json:\n",
      "2016-07-08 03:41:11.675445  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 03:41:11.675493  checking if compiled_dependencies_correct\n",
      "2016-07-08 03:41:11.675510  checking if compiled_input_output_well_defined\n",
      "2016-07-08 03:41:11.675528  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 03:41:11.675809 Compiling executable code into:\n",
      "2016-07-08 03:41:11.676060  run/compiled/code-Makefile.json\n",
      "2016-07-08 03:41:11.676391  run/compiled/code-cmd_extractor.json\n",
      "2016-07-08 03:41:11.676691  run/compiled/code-dataflow_dot.json\n",
      "2016-07-08 03:41:11.677028  run/compiled/code-sql_extractor.json\n",
      "2016-07-08 03:41:11.677414  run/compiled/code-tsX_extractor.json\n",
      "2016-07-08 03:41:11.710729 Validating run/compiled/code-*.json:\n",
      "2016-07-08 03:41:11.737579  checking if codegen_no_path_collision\n",
      "2016-07-08 03:41:11.741145 Generating files:\n",
      "2016-07-08 03:41:11.757999  run/Makefile\n",
      "2016-07-08 03:41:11.758326  run/process/init/app/run.sh\n",
      "2016-07-08 03:41:11.758789  run/process/init/relation/has_spouse/run.sh\n",
      "2016-07-08 03:41:11.759230  run/process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:11.759634  run/dataflow.dot\n",
      "2016-07-08 03:41:11.802697  run/dataflow.pdf\n",
      "2016-07-08 03:41:11.803514   (file:///ConfinedWater/spouse/run/dataflow.pdf)\n",
      "2016-07-08 03:41:11.824102  run/dataflow.svg\n",
      "2016-07-08 03:41:11.824970   (file:///ConfinedWater/spouse/run/dataflow.svg)\n",
      "2016-07-08 03:41:12.046268 ���run/compiled��� -> ���run/compiled~���\n",
      "2016-07-08 03:41:12.048619 ���run/compiled��� -> ���20160708/034110.869722000���\n",
      "���data/articles.done��� -> ���data/articles.done~���\n",
      "���process/init/relation/articles.done��� -> ���process/init/relation/articles.done~���\n",
      "���run/RUNNING��� -> ���20160708/034112.261812000���\n",
      "���run/LATEST��� -> ���20160708/034112.261812000���\n",
      "2016-07-08 03:41:12.466369 #!/bin/sh\n",
      "2016-07-08 03:41:12.466542 # Host: scuba.local\n",
      "2016-07-08 03:41:12.466566 # App: /ConfinedWater/spouse\n",
      "2016-07-08 03:41:12.466608 # Targets: articles\n",
      "2016-07-08 03:41:12.466623 # Plan: run/20160708/034112.261812000/plan.sh\n",
      "2016-07-08 03:41:12.466634 export DEEPDIVE_PWD='/ConfinedWater/spouse'\n",
      "2016-07-08 03:41:12.466682 # execution plan for data/articles\n",
      "2016-07-08 03:41:12.466722 \n",
      "2016-07-08 03:41:12.466751 : ## process/init/app ##########################################################\n",
      "2016-07-08 03:41:12.466765 : # Done: 2016-07-08T03:41:10-0700 (2s ago)\n",
      "2016-07-08 03:41:12.466775 : process/init/app/run.sh\n",
      "2016-07-08 03:41:12.466784 : mark_done process/init/app\n",
      "2016-07-08 03:41:12.466792 : ##############################################################################\n",
      "2016-07-08 03:41:12.466801 \n",
      "2016-07-08 03:41:12.466810 ## process/init/relation/articles ############################################\n",
      "2016-07-08 03:41:12.466825 # Done: 2016-07-08T03:08:15-0700 (32m 57s ago)\n",
      "2016-07-08 03:41:12.466835 process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:12.466844 ++ dirname process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:12.466853 + cd process/init/relation/articles\n",
      "2016-07-08 03:41:12.466862 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/init/relation/articles\n",
      "2016-07-08 03:41:12.466871 + DEEPDIVE_CURRENT_PROCESS_NAME=process/init/relation/articles\n",
      "2016-07-08 03:41:12.466880 + deepdive create table articles\n",
      "2016-07-08 03:41:12.615102 CREATE TABLE\n",
      "2016-07-08 03:41:12.615643 + deepdive load articles\n",
      "2016-07-08 03:41:13.075921 Loading articles from input/articles.tsj.sh (tsj format)\n",
      "2016-07-08 03:41:13.083167 ERROR: Missing /ConfinedWater/spouse/input/signalmedia/signalmedia-1m.jsonl\n",
      "2016-07-08 03:41:13.083231 # Please Download it from http://research.signalmedia.co/newsir16/signal-dataset.html\n",
      "2016-07-08 03:41:13.083253 \n",
      "2016-07-08 03:41:13.083271 # Alternatively, use our sampled data by running:\n",
      "2016-07-08 03:41:13.083288 deepdive load articles input/articles-100.tsv.bz2\n",
      "2016-07-08 03:41:13.083306 \n",
      "2016-07-08 03:41:13.083323 # Or, skipping all NLP markup processes by running:\n",
      "2016-07-08 03:41:13.083340 deepdive create table sentences\n",
      "2016-07-08 03:41:13.083358 deepdive load sentences\n",
      "2016-07-08 03:41:13.083374 deepdive mark done sentences\n",
      "2016-07-08 03:41:13.166411 COPY 0\n",
      "���run/ABORTED��� -> ���20160708/034112.261812000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a sample of 100 and 1000 articles can be downloaded from GitHub and loaded into DeepDive with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   184  100   184    0     0    391      0 --:--:-- --:--:-- --:--:--   392\n",
      "100  135k  100  135k    0     0   101k      0  0:00:01  0:00:01 --:--:--  170k\n",
      "CREATE TABLE\n",
      "Loading articles from input/articles-100.tsj.bz2 (tsj format)\n",
      "COPY 100\n"
     ]
    }
   ],
   "source": [
    "NUM_ARTICLES = 100\n",
    "ARTICLES_FILE = \"articles-%d.tsj.bz2\" % NUM_ARTICLES\n",
    "\n",
    "articles_not_done = !deepdive done articles || date\n",
    "if articles_not_done:\n",
    "    !cd input && curl -RLO \"https://github.com/HazyResearch/deepdive/raw/tab-separated-jsons/examples/spouse/input/\"$ARTICLES_FILE\n",
    "    !deepdive reload articles input/$ARTICLES_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After DeepDive finishes creating the table and then fetching and loading the data, we can take a look at the loaded data using the following `deepdive query` command, which enumerates the values for the `id` column of the `articles` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  id                  \r\n",
      "--------------------------------------\r\n",
      " ba44d0cd-bff2-4875-8036-86f37419b5e7\r\n",
      " c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325\r\n",
      " 0d07e617-00d4-4866-aee2-0ae197ae366f\r\n",
      " ebcd41ea-e5b4-43a4-9e16-4406d81cfcda\r\n",
      " 7516303b-0db5-477d-9e5d-243a73865e39\r\n",
      " f6e047d0-e409-42a6-ab0e-13ab926719a6\r\n",
      " 15d53efb-2151-4164-aee0-cae51faedeeb\r\n",
      " fe6e8fcc-1128-4410-923d-f05c42174336\r\n",
      " 8b31ede3-0f3b-431a-86a3-342ee18cfd83\r\n",
      " 4336860e-fa87-4f54-b3ce-b4afb72c4acd\r\n",
      "(10 rows)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!deepdive query '|10 ?- articles(id, _).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Adding NLP markups\n",
    "Next, we'll use Stanford's [CoreNLP](http://stanfordnlp.github.io/CoreNLP/) natural language processing (NLP) system to add useful markups and structure to our input data.\n",
    "This step will split up our articles into sentences and their component _tokens_ (roughly, the words).\n",
    "Additionally, we'll get _lemmas_ (normalized word forms), _part-of-speech (POS) tags_, _named entity recognition (NER) tags_, and a dependency parse of the sentence.\n",
    "\n",
    "Let's first declare the output schema of this step in `app.ddlog`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "## NLP markup #################################################################\n",
    "sentences(\n",
    "    doc_id         text,\n",
    "    sentence_index int,\n",
    "    tokens         json,\n",
    "    lemmas         json,\n",
    "    pos_tags       json,\n",
    "    ner_tags       json,\n",
    "    doc_offsets    json,\n",
    "    dep_types      json,\n",
    "    dep_tokens     json\n",
    ").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare a DDlog function which takes in the `doc_id` and `content` for an article and returns rows conforming to the sentences schema we just declared, using the **user-defined function (UDF)** in `udf/nlp_markup.sh`.\n",
    "We specify that this `nlp_markup` function should be run over each row from `articles`, and the output appended to `sentences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "function nlp_markup over (\n",
    "        doc_id  text,\n",
    "        content text\n",
    "    ) returns rows like sentences\n",
    "    implementation \"udf/nlp_markup.sh\" handles tsj lines.\n",
    "\n",
    "sentences += nlp_markup(doc_id, content) :-\n",
    "    articles(doc_id, content).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This UDF `udf/nlp_markup.sh` is a Bash script which uses [our own wrapper around CoreNLP](https://github.com/HazyResearch/deepdive/tree/deepdive-corenlp/util/nlp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting udf/nlp_markup.sh\n"
     ]
    }
   ],
   "source": [
    "%%file udf/nlp_markup.sh\n",
    "#!/usr/bin/env bash\n",
    "# Parse documents in tab-separated JSONs input stream with CoreNLP\n",
    "#\n",
    "# $ deepdive corenlp install\n",
    "# $ deepdive corenlp start\n",
    "# $ deepdive env udf/nlp_markup.sh\n",
    "# $ deepdive corenlp stop\n",
    "##\n",
    "set -euo pipefail\n",
    "cd \"$(dirname \"$0\")\"\n",
    "\n",
    "# some configuration knobs for CoreNLP\n",
    ": ${CORENLP_PORT:=$(deepdive corenlp unique-port)}  # a CoreNLP server started ahead of time is shared across parallel UDF processes\n",
    "# See: http://stanfordnlp.github.io/CoreNLP/annotators.html\n",
    ": ${CORENLP_ANNOTATORS:=\"\n",
    "        tokenize\n",
    "        ssplit\n",
    "        pos\n",
    "        ner\n",
    "        lemma\n",
    "        depparse\n",
    "    \"}\n",
    "export CORENLP_PORT\n",
    "export CORENLP_ANNOTATORS\n",
    "\n",
    "# make sure CoreNLP server is available\n",
    "deepdive corenlp is-running || {\n",
    "    echo >&2 \"PLEASE MAKE SURE YOU HAVE RUN: deepdive corenlp start\"\n",
    "    false\n",
    "}\n",
    "\n",
    "# parse input with CoreNLP and output a row for every sentence\n",
    "deepdive corenlp parse-tsj docid+ content=nlp -- docid nlp |\n",
    "deepdive corenlp sentences-tsj docid content:nlp \\\n",
    "                            -- docid nlp.{index,tokens.{word,lemma,pos,ner,characterOffsetBegin}} \\\n",
    "                                     nlp.collapsed-dependencies.{dep_type,dep_token}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we mark it as executable for DeepDive to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x udf/nlp_markup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing this NLP markup step, we need to launch the CoreNLP server in advance, which may take a while to install and load everything.\n",
    "Note that the CoreNLP library requires Java 8 to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoreNLP already installed at /Users/netj/2016/deepdive3/dist/stage/lib/stanford-corenlp/corenlp\n",
      "\u001b[33mCoreNLP server at CORENLP_PORT=10943 already running (PID 38889)\n",
      "\u001b[0mCoreNLP server at CORENLP_PORT=10943 ready.\n",
      "To stop it after final use, run: deepdive corenlp stop\n",
      "To watch its log, run: deepdive corenlp watch-log\n"
     ]
    }
   ],
   "source": [
    "!deepdive corenlp install\n",
    "!deepdive corenlp start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mapp.ddlog: updated since last `deepdive compile`\n",
      "\u001b[0m2016-07-08 03:41:17.836207 ���run/LATEST.COMPILE��� -> ���20160708/034117.795755000���\n",
      "2016-07-08 03:41:17.836417 ���run/RUNNING.COMPILE��� -> ���20160708/034117.795755000���\n",
      "2016-07-08 03:41:17.836439 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 03:41:17.836452  run/compiled/deepdive.conf\n",
      "2016-07-08 03:41:17.836463   from app.ddlog\n",
      "2016-07-08 03:41:18.251606  run/compiled/deepdive.conf.json\n",
      "2016-07-08 03:41:18.379243 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 03:41:18.407712  checking if input_extractors_well_defined\n",
      "2016-07-08 03:41:18.407775  checking if input_schema_wellformed\n",
      "2016-07-08 03:41:18.408233 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 03:41:18.408641  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 03:41:18.421907  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 03:41:18.434907  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 03:41:18.452513  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 03:41:18.465610  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 03:41:18.479069  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 03:41:18.494026  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 03:41:18.515721  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 03:41:18.595769  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 03:41:18.633399  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 03:41:18.647152  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 03:41:18.660390  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 03:41:18.675087  run/compiled/config.json\n",
      "2016-07-08 03:41:18.678528 Validating run/compiled/config.json:\n",
      "2016-07-08 03:41:18.707710  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 03:41:18.707766  checking if compiled_dependencies_correct\n",
      "2016-07-08 03:41:18.707786  checking if compiled_input_output_well_defined\n",
      "2016-07-08 03:41:18.707804  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 03:41:18.708019 Compiling executable code into:\n",
      "2016-07-08 03:41:18.708302  run/compiled/code-Makefile.json\n",
      "2016-07-08 03:41:18.708620  run/compiled/code-cmd_extractor.json\n",
      "2016-07-08 03:41:18.708906  run/compiled/code-dataflow_dot.json\n",
      "2016-07-08 03:41:18.709389  run/compiled/code-sql_extractor.json\n",
      "2016-07-08 03:41:18.709832  run/compiled/code-tsX_extractor.json\n",
      "2016-07-08 03:41:18.747969 Validating run/compiled/code-*.json:\n",
      "2016-07-08 03:41:18.775568  checking if codegen_no_path_collision\n",
      "2016-07-08 03:41:18.779062 Generating files:\n",
      "2016-07-08 03:41:18.795357  run/Makefile\n",
      "2016-07-08 03:41:18.795766  run/process/init/app/run.sh\n",
      "2016-07-08 03:41:18.796411  run/process/init/relation/has_spouse/run.sh\n",
      "2016-07-08 03:41:18.796731  run/process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:18.797378  run/dataflow.dot\n",
      "2016-07-08 03:41:18.798016  run/process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:18.839112  run/dataflow.pdf\n",
      "2016-07-08 03:41:18.839982   (file:///ConfinedWater/spouse/run/dataflow.pdf)\n",
      "2016-07-08 03:41:18.861458  run/dataflow.svg\n",
      "2016-07-08 03:41:18.862506   (file:///ConfinedWater/spouse/run/dataflow.svg)\n",
      "2016-07-08 03:41:19.087431 ���run/compiled��� -> ���run/compiled~���\n",
      "2016-07-08 03:41:19.090435 ���run/compiled��� -> ���20160708/034117.795755000���\n",
      "���data/sentences.done��� -> ���data/sentences.done~���\n",
      "���run/RUNNING��� -> ���20160708/034119.286975000���\n",
      "���run/LATEST��� -> ���20160708/034119.286975000���\n",
      "2016-07-08 03:41:19.502093 #!/bin/sh\n",
      "2016-07-08 03:41:19.502286 # Host: scuba.local\n",
      "2016-07-08 03:41:19.502328 # App: /ConfinedWater/spouse\n",
      "2016-07-08 03:41:19.502351 # Targets: sentences\n",
      "2016-07-08 03:41:19.502373 # Plan: run/20160708/034119.286975000/plan.sh\n",
      "2016-07-08 03:41:19.502391 export DEEPDIVE_PWD='/ConfinedWater/spouse'\n",
      "2016-07-08 03:41:19.502409 # execution plan for data/sentences\n",
      "2016-07-08 03:41:19.502425 \n",
      "2016-07-08 03:41:19.502447 : ## process/init/app ##########################################################\n",
      "2016-07-08 03:41:19.502475 : # Done: 2016-07-08T03:41:10-0700 (9s ago)\n",
      "2016-07-08 03:41:19.502494 : process/init/app/run.sh\n",
      "2016-07-08 03:41:19.502507 : mark_done process/init/app\n",
      "2016-07-08 03:41:19.502525 : ##############################################################################\n",
      "2016-07-08 03:41:19.502537 \n",
      "2016-07-08 03:41:19.502548 : ## process/init/relation/articles ############################################\n",
      "2016-07-08 03:41:19.502558 : # Done: 2016-07-08T03:41:15-0700 (4s ago)\n",
      "2016-07-08 03:41:19.502570 : process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:19.502589 : mark_done process/init/relation/articles\n",
      "2016-07-08 03:41:19.502623 : ##############################################################################\n",
      "2016-07-08 03:41:19.502644 \n",
      "2016-07-08 03:41:19.502656 : ## data/articles #############################################################\n",
      "2016-07-08 03:41:19.502680 : # Done: 2016-07-08T03:41:15-0700 (4s ago)\n",
      "2016-07-08 03:41:19.502722 : # no-op\n",
      "2016-07-08 03:41:19.502745 : mark_done data/articles\n",
      "2016-07-08 03:41:19.502763 : ##############################################################################\n",
      "2016-07-08 03:41:19.502781 \n",
      "2016-07-08 03:41:19.502801 ## process/ext_sentences_by_nlp_markup #######################################\n",
      "2016-07-08 03:41:19.502818 # Done: N/A\n",
      "2016-07-08 03:41:19.502829 process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:19.502840 ++ dirname process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:19.502849 + cd process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:41:19.502860 + : dd_tmp_ dd_old_\n",
      "2016-07-08 03:41:19.502870 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:41:19.502881 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:41:19.502891 + export DEEPDIVE_LOAD_FORMAT=tsj\n",
      "2016-07-08 03:41:19.502901 + DEEPDIVE_LOAD_FORMAT=tsj\n",
      "2016-07-08 03:41:19.502932 + output_relation=sentences\n",
      "2016-07-08 03:41:19.502967 + output_relation_tmp=dd_tmp_sentences\n",
      "2016-07-08 03:41:19.502991 + output_relation_old=dd_old_sentences\n",
      "2016-07-08 03:41:19.503008 + deepdive create table-if-not-exists sentences\n",
      "2016-07-08 03:41:19.693869 CREATE TABLE\n",
      "2016-07-08 03:41:19.694766 + deepdive create table dd_tmp_sentences like sentences\n",
      "2016-07-08 03:41:19.905413 CREATE TABLE\n",
      "2016-07-08 03:41:19.906353 + deepdive compute execute 'input_sql=\n",
      "2016-07-08 03:41:19.906386 \n",
      "2016-07-08 03:41:19.906401 SELECT R0.id AS column_0\n",
      "2016-07-08 03:41:19.906420      , R0.content AS column_1\n",
      "2016-07-08 03:41:19.906440 FROM articles R0\n",
      "2016-07-08 03:41:19.906459 \n",
      "2016-07-08 03:41:19.906480 ' 'command=cd \"$DEEPDIVE_APP\" && udf/nlp_markup.sh' output_relation=dd_tmp_sentences\n",
      "2016-07-08 03:41:20.079254 Executing with the following configuration:\n",
      "2016-07-08 03:41:20.079316  DEEPDIVE_NUM_PROCESSES=7\n",
      "2016-07-08 03:41:20.079330  DEEPDIVE_NUM_PARALLEL_UNLOADS=1\n",
      "2016-07-08 03:41:20.079340  DEEPDIVE_NUM_PARALLEL_LOADS=1\n",
      "2016-07-08 03:41:20.219922 unloading to feed_processes-1: '\n",
      "2016-07-08 03:41:20.220021 \n",
      "2016-07-08 03:41:20.220070 SELECT R0.id AS column_0\n",
      "2016-07-08 03:41:20.220094      , R0.content AS column_1\n",
      "2016-07-08 03:41:20.220119 FROM articles R0\n",
      "2016-07-08 03:41:20.220139 \n",
      "2016-07-08 03:41:20.220158 '\n",
      "2016-07-08 03:41:20.620186 Loading dd_tmp_sentences from output_computed-1 (tsj format)\n",
      "2016-07-08 03:41:20.672284 Parsing \"fe6e8fcc-1128-4410-923d-f05c42174336\"\n",
      "2016-07-08 03:41:20.678437 Parsing \"0d07e617-00d4-4866-aee2-0ae197ae366f\"\n",
      "2016-07-08 03:41:20.681041 Parsing \"ba44d0cd-bff2-4875-8036-86f37419b5e7\"\n",
      "2016-07-08 03:41:20.683267 Parsing \"ebcd41ea-e5b4-43a4-9e16-4406d81cfcda\"\n",
      "2016-07-08 03:41:20.685261 Parsing \"c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325\"\n",
      "2016-07-08 03:41:20.686516 Parsing \"f6e047d0-e409-42a6-ab0e-13ab926719a6\"\n",
      "2016-07-08 03:41:20.687221 Parsing \"15d53efb-2151-4164-aee0-cae51faedeeb\"\n",
      "2016-07-08 03:41:20.957306 Parsing \"df13cc43-53fd-4f09-9a7e-d69b12a4adc0\"\n",
      "2016-07-08 03:41:21.691072 Parsing \"c27a162d-f2d1-4bdb-84ba-0915a082775b\"\n",
      "2016-07-08 03:41:21.860955 Parsing \"46c3af65-1da9-459b-a98d-6f0eae12577e\"\n",
      "2016-07-08 03:41:21.877602 Parsing \"8b31ede3-0f3b-431a-86a3-342ee18cfd83\"\n",
      "2016-07-08 03:41:22.050714 Parsing \"4336860e-fa87-4f54-b3ce-b4afb72c4acd\"\n",
      "2016-07-08 03:41:22.133488 Parsing \"7516303b-0db5-477d-9e5d-243a73865e39\"\n",
      "2016-07-08 03:41:22.136967 Parsing \"43c02bac-d556-4851-9b04-7773700759b6\"\n",
      "2016-07-08 03:41:22.189899 Parsing \"a6f1a6ee-6e47-4a01-b88d-e14ec57a3ec4\"\n",
      "2016-07-08 03:41:22.436955 Parsing \"b4968e78-ec5a-466e-863f-fef18e8ae99d\"\n",
      "2016-07-08 03:41:22.474587 Parsing \"f642dff5-2cd3-46a0-b530-792529a8ebb2\"\n",
      "2016-07-08 03:41:22.730169 Parsing \"d63353aa-58f1-413d-8f60-fac44c41d4b7\"\n",
      "2016-07-08 03:41:23.250922 Parsing \"5dd9bf47-c8a9-49e3-8d02-994f8eabb91a\"\n",
      "2016-07-08 03:41:23.280507 Parsing \"5e7a035e-3a52-4133-afb7-564423d6b1b0\"\n",
      "2016-07-08 03:41:23.307403 Parsing \"70ecb3ad-cbbe-4097-8608-e3373a34a728\"\n",
      "2016-07-08 03:41:23.504793 Parsing \"693ae030-4239-4291-b248-dbf7c1696ff2\"\n",
      "2016-07-08 03:41:23.518021 Parsing \"8bd53371-3b63-4bb0-acc7-5e2c2ecc8ff4\"\n",
      "2016-07-08 03:41:23.824832 Parsing \"0a74a914-54fb-47bc-acae-5dcd10ed5c3d\"\n",
      "2016-07-08 03:41:23.999714 Parsing \"e4304e4e-a09f-4053-acb4-f1042e5f132e\"\n",
      "2016-07-08 03:41:24.195750 Parsing \"8232e1c2-f5f2-4b86-88d0-4ec5a420dced\"\n",
      "2016-07-08 03:41:24.361820 Parsing \"36349778-9942-475d-bdf2-23b7372911c1\"\n",
      "2016-07-08 03:41:24.579510 Parsing \"f18bbd65-57a4-414a-b78b-356b9ecb65f0\"\n",
      "2016-07-08 03:41:24.750887 Parsing \"db60062c-cd21-40a5-ab3b-648d5a320bf2\"\n",
      "2016-07-08 03:41:24.759447 Parsing \"9a95041a-90d6-43f1-b015-89e99437c452\"\n",
      "2016-07-08 03:41:24.765888 Parsing \"328623e0-52f3-44a6-b66b-496cd9d93762\"\n",
      "2016-07-08 03:41:24.900142 Parsing \"9b28e780-ba48-4a53-8682-7c58c141a1b6\"\n",
      "2016-07-08 03:41:25.149284 Parsing \"49011354-37d5-4154-8b4c-f96e7939fb4e\"\n",
      "2016-07-08 03:41:25.316492 Parsing \"7d4928a5-34fd-4d1c-894f-8e165fcf61aa\"\n",
      "2016-07-08 03:41:25.447840 Parsing \"8d8a6a7e-26c6-40e1-a651-b9517f9c6f5a\"\n",
      "2016-07-08 03:41:25.537997 Parsing \"0d31634e-7d2a-4dfd-b7dc-bb79fe691cb3\"\n",
      "2016-07-08 03:41:25.942810 Parsing \"0e1d09ed-6ba7-430b-be20-0a3744bfd7b0\"\n",
      "2016-07-08 03:41:26.108458 Parsing \"bd104c74-afa6-4d79-bf24-3249c7643130\"\n",
      "2016-07-08 03:41:26.323555 Parsing \"23490793-bb60-44c0-bbec-9c3be871d762\"\n",
      "2016-07-08 03:41:26.330816 Parsing \"9c72b6ec-8f72-4914-8ca1-21935e1e2986\"\n",
      "2016-07-08 03:41:26.342485 Parsing \"f56cc7a5-dd56-47a5-8318-4068bb8a1c9f\"\n",
      "2016-07-08 03:41:26.656078 Parsing \"f69ff4d5-ebaf-4c85-a3c1-4f185887e57c\"\n",
      "2016-07-08 03:41:26.677312 Parsing \"18658e4a-a94e-478f-ab2e-2ee709bd47e5\"\n",
      "2016-07-08 03:41:26.953480 Parsing \"328b5f1c-2b52-4eac-916c-c7983d4882a4\"\n",
      "2016-07-08 03:41:27.006891 Parsing \"2dea8451-cbcd-42b9-a263-ffc3397bde43\"\n",
      "2016-07-08 03:41:27.008151 Parsing \"98699d61-00cb-423f-8851-e83b348cc845\"\n",
      "2016-07-08 03:41:27.102713 Parsing \"fc6ad33a-ae70-41ff-9f0e-7283f85878f6\"\n",
      "2016-07-08 03:41:27.161206 Parsing \"43859f9c-178c-4df5-9bc7-af2aa5c3a57f\"\n",
      "2016-07-08 03:41:27.174058 Parsing \"e505b558-5db9-4d92-aec0-3b32341499b8\"\n",
      "2016-07-08 03:41:27.240042 Parsing \"acedaa54-9820-4b71-aa7b-38dc7ed1d2a6\"\n",
      "2016-07-08 03:41:27.926144 Parsing \"e2fac553-db94-482f-bd60-1889853028da\"\n",
      "2016-07-08 03:41:28.045798 Parsing \"fa128c30-94fe-43a9-a269-b646d0cbb83e\"\n",
      "2016-07-08 03:41:28.072716 Parsing \"eacc9625-b22d-4a44-a62e-7d53c132af1a\"\n",
      "2016-07-08 03:41:28.341686 Parsing \"9d486048-bcc2-488c-be8a-32d7913c3b3b\"\n",
      "2016-07-08 03:41:28.648465 Parsing \"d6880afb-7fcb-4576-9d17-cedd343677f9\"\n",
      "2016-07-08 03:41:28.960668 Parsing \"d8ec5d11-bf4d-4464-8dc7-c0984563181b\"\n",
      "2016-07-08 03:41:29.110334 Parsing \"981d477d-469f-4b90-906d-214d63506beb\"\n",
      "2016-07-08 03:41:29.147736 Parsing \"08679f4f-0b8b-452b-8cca-4cb538993417\"\n",
      "2016-07-08 03:41:29.210293 Parsing \"0074179e-cac1-409c-8dbc-598877434181\"\n",
      "2016-07-08 03:41:29.379451 Parsing \"4534295d-c5f0-47d1-91d9-dcd65f45bf6a\"\n",
      "2016-07-08 03:41:29.546886 Parsing \"e70f708c-eb4f-4a8f-97ec-8f50e865ad06\"\n",
      "2016-07-08 03:41:29.827731 Parsing \"d969e80b-6b4a-4cda-ba5c-18c46b0fbd39\"\n",
      "2016-07-08 03:41:29.838868 Parsing \"c4a1f668-b653-45d9-a95a-bb692c3e81cb\"\n",
      "2016-07-08 03:41:30.059179 Parsing \"68145e40-fdaf-4244-87b0-942740eba969\"\n",
      "2016-07-08 03:41:30.145811 Parsing \"ec1133a0-b707-4eda-a819-de0bb47180fe\"\n",
      "2016-07-08 03:41:30.166123 Parsing \"7e5f4072-b69f-4819-8ed6-62bdd0100621\"\n",
      "2016-07-08 03:41:30.271486 Parsing \"82c0c1d4-45b1-4861-b880-c414de299948\"\n",
      "2016-07-08 03:41:30.274093 Parsing \"37cb369b-c6b4-42ba-939a-d77e5341686e\"\n",
      "2016-07-08 03:41:30.541561 Parsing \"3deea828-e3a1-4c4d-9a90-482cabc020d8\"\n",
      "2016-07-08 03:41:30.739770 Parsing \"ddf3dfd2-cc21-46ca-a0e7-bd66eeb6bec6\"\n",
      "2016-07-08 03:41:30.761354 Parsing \"625eea20-f3ec-4a0f-864f-ffcd9308fffd\"\n",
      "2016-07-08 03:41:30.788982 Parsing \"14c446d0-bce8-47d3-ab97-583aee3bd859\"\n",
      "2016-07-08 03:41:30.869209 Parsing \"dbc798be-9a6e-48b7-8721-31f84e89c10b\"\n",
      "2016-07-08 03:41:31.067740 Parsing \"d2c738c5-70b3-49a6-8c62-444896f34c26\"\n",
      "2016-07-08 03:41:31.092616 Parsing \"507d552f-db6a-495e-b78a-95783cad9af1\"\n",
      "2016-07-08 03:41:31.276343 Parsing \"340bb625-bb7e-49af-aa8d-781e5762f7a3\"\n",
      "2016-07-08 03:41:31.381388 Parsing \"431dfaf8-cea8-4b80-a466-819fa473e69f\"\n",
      "2016-07-08 03:41:31.614134 Parsing \"3011fed2-1784-47f8-834c-c1f1af79e476\"\n",
      "2016-07-08 03:41:31.802391 Parsing \"08c9af8e-b648-4e8d-a1ba-5cda48caa68d\"\n",
      "2016-07-08 03:41:31.990976 Parsing \"4fb3b88d-4000-4a11-bb77-8008358fe6d7\"\n",
      "2016-07-08 03:41:32.083827 Parsing \"dd970ef6-ebd2-4be8-9226-7a3a6129891a\"\n",
      "2016-07-08 03:41:32.111909 Parsing \"183de50d-7772-44a2-9801-0739ef896f3e\"\n",
      "2016-07-08 03:41:32.379214 Parsing \"487361e3-7385-4d70-a010-abc0296614f9\"\n",
      "2016-07-08 03:41:32.409836 Parsing \"98db5e18-4e0b-4152-b9d5-a1891c257021\"\n",
      "2016-07-08 03:41:32.646295 Parsing \"d611cfe5-1bb1-429f-8440-1c4553f94805\"\n",
      "2016-07-08 03:41:32.661532 Parsing \"e288568e-465a-40f0-aa78-a31c07aa7d6d\"\n",
      "2016-07-08 03:41:32.773115 Parsing \"eb8b7cb1-e1a8-4dfe-b519-cf8a3554c37c\"\n",
      "2016-07-08 03:41:32.926366 Parsing \"a46ee549-8937-445d-a68f-fbcd619ae30f\"\n",
      "2016-07-08 03:41:32.971196 Parsing \"bbd65b1c-c3ee-4a7d-a80f-7b091282a053\"\n",
      "2016-07-08 03:41:33.125064 Parsing \"6779b9e1-073a-4adb-a20d-7d11c61410c9\"\n",
      "2016-07-08 03:41:33.192779 Parsing \"e7ea7dd7-0266-431a-bfb4-c8e88532c0d4\"\n",
      "2016-07-08 03:41:33.220994 Parsing \"c902ad3d-3798-4c00-b09c-c9079de7bf49\"\n",
      "2016-07-08 03:41:33.416653 Parsing \"9662058b-fca5-4771-8058-c7fd7bd548a3\"\n",
      "2016-07-08 03:41:33.496392 Parsing \"ac6492fb-c67d-491b-b401-c81a21d4a482\"\n",
      "2016-07-08 03:41:33.638295 Parsing \"79205745-b593-4b98-8a94-da6b8238fefc\"\n",
      "2016-07-08 03:41:33.668492 Parsing \"172960c6-cb26-4cd1-99a8-d7cb92f8dec8\"\n",
      "2016-07-08 03:41:33.808226 Parsing \"ed17304d-81f1-4329-bc5b-ac381e5223e5\"\n",
      "2016-07-08 03:41:33.949645 Parsing \"60454b80-33e5-4115-8975-93306275856b\"\n",
      "2016-07-08 03:41:34.663566 Parsing \"4a58fe6d-57ca-4191-bfa7-e1dbf7db65c8\"\n",
      "2016-07-08 03:41:34.708147 Parsing \"3e8bf5da-cb3f-4816-8fc8-914a8e22ee2c\"\n",
      "2016-07-08 03:41:37.700748 COPY 3831\n",
      "2016-07-08 03:41:37.706629 + : 'Replacing sentences with dd_tmp_sentences'\n",
      "2016-07-08 03:41:37.706709 + deepdive sql 'DROP TABLE IF EXISTS dd_old_sentences CASCADE;'\n",
      "2016-07-08 03:41:37.779308 DROP TABLE\n",
      "2016-07-08 03:41:37.780116 + deepdive sql 'ALTER TABLE sentences     RENAME TO dd_old_sentences;'\n",
      "2016-07-08 03:41:37.862109 ALTER TABLE\n",
      "2016-07-08 03:41:37.862831 + deepdive sql 'ALTER TABLE dd_tmp_sentences RENAME TO sentences;'\n",
      "2016-07-08 03:41:37.935689 ALTER TABLE\n",
      "2016-07-08 03:41:37.936350 + deepdive sql 'DROP TABLE IF EXISTS dd_old_sentences CASCADE;'\n",
      "2016-07-08 03:41:38.010649 DROP TABLE\n",
      "2016-07-08 03:41:38.011323 + deepdive db analyze sentences\n",
      "2016-07-08 03:41:38.119637 ANALYZE\n",
      "2016-07-08 03:41:38.121177 mark_done process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:41:38.134992 ##############################################################################\n",
      "2016-07-08 03:41:38.135033 \n",
      "2016-07-08 03:41:38.135047 ## data/sentences ############################################################\n",
      "2016-07-08 03:41:38.135059 # Done: 2016-07-08T03:25:44-0700 (15m 35s ago)\n",
      "2016-07-08 03:41:38.135070 # no-op\n",
      "2016-07-08 03:41:38.135081 mark_done data/sentences\n",
      "2016-07-08 03:41:38.147940 ##############################################################################\n",
      "2016-07-08 03:41:38.148000 \n",
      "���run/FINISHED��� -> ���run/FINISHED~���\n",
      "���run/FINISHED��� -> ���20160708/034119.286975000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we take a look at a sample of the NLP markups, they will have tokens and NER tags that look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                doc_id                | index |                                                                                                                                                                                                                                tokens                                                                                                                                                                                                                                |                                                                                                                                                               ner_tags                                                                                                                                                                \n",
      "--------------------------------------+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " 15d53efb-2151-4164-aee0-cae51faedeeb |     0 | [\",\",\"6:21\",\"PM\",\"ET\",\"-RRB-\",\"--\",\"Construction\",\"vehicle\",\"company\",\"Caterpillar\",\"is\",\"still\",\"on\",\"the\",\"hook\",\"for\",\"$\",\"4.5\",\"million\",\"in\",\"damages\",\"after\",\"the\",\"Washington\",\"State\",\"Court\",\"of\",\"Appeals\",\"affirmed\",\"a\",\"jury\",\"verdict\",\"in\",\"favor\",\"of\",\"the\",\"wife\",\"of\",\"a\",\"construction\",\"worker\",\"who\",\"sued\",\"the\",\"company\",\"and\",\"others\",\"on\",\"claims\",\"stemming\",\"from\",\"her\",\"husband\",\"'s\",\"death\",\"due\",\"to\",\"asbestos\",\"exposure\",\".\"] | [\"O\",\"TIME\",\"TIME\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"ORGANIZATION\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"MONEY\",\"MONEY\",\"MONEY\",\"O\",\"O\",\"O\",\"O\",\"ORGANIZATION\",\"ORGANIZATION\",\"ORGANIZATION\",\"ORGANIZATION\",\"ORGANIZATION\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]\n",
      " 15d53efb-2151-4164-aee0-cae51faedeeb |     1 | [\"The\",\"three-judge\",\"panel\",\"affirmed\",\"the\",\"judgment\",\"on\",\"product\",\"liability\",\",\",\"failure\",\"to\",\"warn\",\"and\",\"negligence\",\"claims\",\",\",\"finding\",\"that\",\"Betty\",\"Estenson\",\"and\",\"her\",\"husband\",\"Edwin\",\",\",\"before\",\"he\",\"died\",\"of\",\"mesothelioma\",\"in\",\"February\",\"2012\",\",\",\"had\",\"presented\",\"sufficient\",\"evidence\",\"...\"]                                                                                                                             | [\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"PERSON\",\"PERSON\",\"O\",\"O\",\"O\",\"PERSON\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"DATE\",\"DATE\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]\n",
      " df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |     0 | [\"Dame\",\"Joan\",\"Collins\",\"said\",\"she\",\"used\",\"to\",\"``\",\"nag\",\"''\",\"her\",\"sister\",\"Jackie\",\"about\",\"getting\",\"mammograms\",\"but\",\"she\",\"``\",\"refused\",\"''\",\".\"]                                                                                                                                                                                                                                                                                                        | [\"PERSON\",\"PERSON\",\"PERSON\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"PERSON\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]\n",
      " df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |     1 | [\"Jackie\",\",\",\"who\",\"sold\",\"more\",\"than\",\"500\",\"million\",\"novels\",\"in\",\"more\",\"than\",\"40\",\"countries\",\"in\",\"her\",\"four\",\"decades-long\",\"career\",\"as\",\"a\",\"writer\",\"in\",\"raunchy\",\"female\",\"fiction\",\",\",\"died\",\"of\",\"breast\",\"cancer\",\"last\",\"Saturday\",\"aged\",\"77\",\".\"]                                                                                                                                                                                             | [\"PERSON\",\"O\",\"O\",\"O\",\"O\",\"O\",\"NUMBER\",\"NUMBER\",\"O\",\"O\",\"O\",\"O\",\"NUMBER\",\"O\",\"O\",\"O\",\"NUMBER\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"DATE\",\"DATE\",\"O\",\"NUMBER\",\"O\"]\n",
      " df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |     2 | [\"Joan\",\"and\",\"Jackie\",\"Collins\",\"during\",\"the\",\"filming\",\"of\",\"the\",\"2013\",\"New\",\"Year\",\"'s\",\"Eve\",\"Graham\",\"Norton\",\"Show\",\"Dame\",\"Joan\",\"said\",\"she\",\"and\",\"her\",\"sister\",\"were\",\"``\",\"thick\",\"as\",\"thieves\",\"''\",\"but\",\"admitted\",\"they\",\"did\",\"have\",\"``\",\"estrangements\",\"''\",\"in\",\"the\",\"past\",\".\"]                                                                                                                                                           | [\"PERSON\",\"O\",\"PERSON\",\"PERSON\",\"O\",\"O\",\"O\",\"O\",\"DATE\",\"DATE\",\"DATE\",\"DATE\",\"DATE\",\"DATE\",\"PERSON\",\"PERSON\",\"PERSON\",\"PERSON\",\"PERSON\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"DATE\",\"DATE\",\"O\"]\n",
      "(5 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "deepdive query '\n",
    "    doc_id, index, tokens, ner_tags | 5\n",
    "    ?- sentences(doc_id, index, tokens, lemmas, pos_tags, ner_tags, _, _, _).\n",
    "'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extracting candidate relation mentions\n",
    "\n",
    "#### Mentions of people\n",
    "Once again we first declare the schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "## Candidate mapping ##########################################################\n",
    "person_mention(\n",
    "    mention_id     text,\n",
    "    mention_text   text,\n",
    "    doc_id         text,\n",
    "    sentence_index int,\n",
    "    begin_index    int,\n",
    "    end_index      int\n",
    ").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be storing each person as a row referencing a sentence with beginning and ending indexes.\n",
    "Again, we next declare a function that references a UDF and takes as input the sentence tokens and NER tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "function map_person_mention over (\n",
    "        doc_id         text,\n",
    "        sentence_index int,\n",
    "        tokens         text[],\n",
    "        ner_tags       text[]\n",
    "    ) returns rows like person_mention\n",
    "    implementation \"udf/map_person_mention.py\" handles tsj lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a simple UDF in Python that will tag spans of contiguous tokens with the NER tag `PERSON` as person mentions (i.e., we'll essentially rely on CoreNLP's NER module).\n",
    "Note that we've already used a Bash script as a UDF, and indeed any programming language can be used.\n",
    "(DeepDive will just check the path specified in the top line, e.g., `#!/usr/bin/env python`.)\n",
    "However, DeepDive provides some convenient utilities for Python UDFs which handle all IO encoding/decoding.\n",
    "To write our UDF `udf/map_person_mention.py`, we'll start by specifying that our UDF will handle TSV lines (as specified in the DDlog above).\n",
    "Additionally, we'll specify the exact type schema of both input and output, which DeepDive will check for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting udf/map_person_mention.py\n"
     ]
    }
   ],
   "source": [
    "%%file udf/map_person_mention.py\n",
    "#!/usr/bin/env python\n",
    "from deepdive import *\n",
    "\n",
    "@tsj_extractor\n",
    "@returns(lambda\n",
    "        mention_id       = \"text\",\n",
    "        mention_text     = \"text\",\n",
    "        doc_id           = \"text\",\n",
    "        sentence_index   = \"int\",\n",
    "        begin_index      = \"int\",\n",
    "        end_index        = \"int\",\n",
    "    :[])\n",
    "def extract(\n",
    "        doc_id         = \"text\",\n",
    "        sentence_index = \"int\",\n",
    "        tokens         = \"text[]\",\n",
    "        ner_tags       = \"text[]\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Finds phrases that are continuous words tagged with PERSON.\n",
    "    \"\"\"\n",
    "    num_tokens = len(ner_tags)\n",
    "    # find all first indexes of series of tokens tagged as PERSON\n",
    "    first_indexes = (i for i in xrange(num_tokens) if ner_tags[i] == \"PERSON\" and (i == 0 or ner_tags[i-1] != \"PERSON\"))\n",
    "    for begin_index in first_indexes:\n",
    "        # find the end of the PERSON phrase (consecutive tokens tagged as PERSON)\n",
    "        end_index = begin_index + 1\n",
    "        while end_index < num_tokens and ner_tags[end_index] == \"PERSON\":\n",
    "            end_index += 1\n",
    "        end_index -= 1\n",
    "        # generate a mention identifier\n",
    "        mention_id = \"%s_%d_%d_%d\" % (doc_id, sentence_index, begin_index, end_index)\n",
    "        mention_text = \" \".join(map(lambda i: tokens[i], xrange(begin_index, end_index + 1)))\n",
    "        # Output a tuple for each PERSON phrase\n",
    "        yield [\n",
    "            mention_id,\n",
    "            mention_text,\n",
    "            doc_id,\n",
    "            sentence_index,\n",
    "            begin_index,\n",
    "            end_index,\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x udf/map_person_mention.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we write a simple function which extracts and tags all subsequences of tokens having the NER tag \"PERSON\".\n",
    "Note that the `extract` function must be a generator (i.e., use a `yield` statement to return output rows).\n",
    "\n",
    "Finally, we specify that the function will be applied to rows from the `sentences` table and append to the `person_mention` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "person_mention += map_person_mention(\n",
    "    doc_id, sentence_index, tokens, ner_tags\n",
    ") :-\n",
    "    sentences(doc_id, sentence_index, tokens, _, _, ner_tags, _, _, _).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to run, just compile and execute as in previous steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mapp.ddlog: updated since last `deepdive compile`\n",
      "\u001b[0m2016-07-08 03:41:39.137785 ���run/LATEST.COMPILE��� -> ���20160708/034139.098578000���\n",
      "2016-07-08 03:41:39.138109 ���run/RUNNING.COMPILE��� -> ���20160708/034139.098578000���\n",
      "2016-07-08 03:41:39.138127 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 03:41:39.138138  run/compiled/deepdive.conf\n",
      "2016-07-08 03:41:39.138153   from app.ddlog\n",
      "2016-07-08 03:41:39.552891  run/compiled/deepdive.conf.json\n",
      "2016-07-08 03:41:39.697039 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 03:41:39.725289  checking if input_extractors_well_defined\n",
      "2016-07-08 03:41:39.725361  checking if input_schema_wellformed\n",
      "2016-07-08 03:41:39.725592 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 03:41:39.726008  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 03:41:39.740667  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 03:41:39.752661  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 03:41:39.770757  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 03:41:39.784363  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 03:41:39.798841  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 03:41:39.813823  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 03:41:39.834348  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 03:41:39.911740  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 03:41:39.946408  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 03:41:39.960982  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 03:41:39.975288  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 03:41:39.989909  run/compiled/config.json\n",
      "2016-07-08 03:41:39.992427 Validating run/compiled/config.json:\n",
      "2016-07-08 03:41:40.022808  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 03:41:40.022870  checking if compiled_dependencies_correct\n",
      "2016-07-08 03:41:40.022885  checking if compiled_input_output_well_defined\n",
      "2016-07-08 03:41:40.022898  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 03:41:40.023189 Compiling executable code into:\n",
      "2016-07-08 03:41:40.023500  run/compiled/code-Makefile.json\n",
      "2016-07-08 03:41:40.023858  run/compiled/code-cmd_extractor.json\n",
      "2016-07-08 03:41:40.024155  run/compiled/code-dataflow_dot.json\n",
      "2016-07-08 03:41:40.024645  run/compiled/code-sql_extractor.json\n",
      "2016-07-08 03:41:40.025099  run/compiled/code-tsX_extractor.json\n",
      "2016-07-08 03:41:40.055216 Validating run/compiled/code-*.json:\n",
      "2016-07-08 03:41:40.080513  checking if codegen_no_path_collision\n",
      "2016-07-08 03:41:40.083855 Generating files:\n",
      "2016-07-08 03:41:40.099926  run/Makefile\n",
      "2016-07-08 03:41:40.100190  run/process/init/app/run.sh\n",
      "2016-07-08 03:41:40.100481  run/process/init/relation/has_spouse/run.sh\n",
      "2016-07-08 03:41:40.100878  run/process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:40.101272  run/dataflow.dot\n",
      "2016-07-08 03:41:40.101830  run/process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:40.102275  run/process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:41:40.144499  run/dataflow.pdf\n",
      "2016-07-08 03:41:40.145331   (file:///ConfinedWater/spouse/run/dataflow.pdf)\n",
      "2016-07-08 03:41:40.165568  run/dataflow.svg\n",
      "2016-07-08 03:41:40.166435   (file:///ConfinedWater/spouse/run/dataflow.svg)\n",
      "2016-07-08 03:41:40.384701 ���run/compiled��� -> ���run/compiled~���\n",
      "2016-07-08 03:41:40.387272 ���run/compiled��� -> ���20160708/034139.098578000���\n",
      "���data/person_mention.done��� -> ���data/person_mention.done~���\n",
      "���run/RUNNING��� -> ���20160708/034140.582441000���\n",
      "���run/LATEST��� -> ���20160708/034140.582441000���\n",
      "2016-07-08 03:41:40.830091 #!/bin/sh\n",
      "2016-07-08 03:41:40.830256 # Host: scuba.local\n",
      "2016-07-08 03:41:40.830274 # App: /ConfinedWater/spouse\n",
      "2016-07-08 03:41:40.830285 # Targets: person_mention\n",
      "2016-07-08 03:41:40.830295 # Plan: run/20160708/034140.582441000/plan.sh\n",
      "2016-07-08 03:41:40.830306 export DEEPDIVE_PWD='/ConfinedWater/spouse'\n",
      "2016-07-08 03:41:40.830316 # execution plan for data/person_mention\n",
      "2016-07-08 03:41:40.830326 \n",
      "2016-07-08 03:41:40.830336 : ## process/init/app ##########################################################\n",
      "2016-07-08 03:41:40.830346 : # Done: 2016-07-08T03:41:10-0700 (30s ago)\n",
      "2016-07-08 03:41:40.830356 : process/init/app/run.sh\n",
      "2016-07-08 03:41:40.830366 : mark_done process/init/app\n",
      "2016-07-08 03:41:40.830375 : ##############################################################################\n",
      "2016-07-08 03:41:40.830385 \n",
      "2016-07-08 03:41:40.830395 : ## process/init/relation/articles ############################################\n",
      "2016-07-08 03:41:40.830405 : # Done: 2016-07-08T03:41:15-0700 (25s ago)\n",
      "2016-07-08 03:41:40.830415 : process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:40.830424 : mark_done process/init/relation/articles\n",
      "2016-07-08 03:41:40.830434 : ##############################################################################\n",
      "2016-07-08 03:41:40.830444 \n",
      "2016-07-08 03:41:40.830453 : ## data/articles #############################################################\n",
      "2016-07-08 03:41:40.830463 : # Done: 2016-07-08T03:41:15-0700 (25s ago)\n",
      "2016-07-08 03:41:40.830472 : # no-op\n",
      "2016-07-08 03:41:40.830481 : mark_done data/articles\n",
      "2016-07-08 03:41:40.830491 : ##############################################################################\n",
      "2016-07-08 03:41:40.830500 \n",
      "2016-07-08 03:41:40.830509 : ## process/ext_sentences_by_nlp_markup #######################################\n",
      "2016-07-08 03:41:40.830519 : # Done: 2016-07-08T03:41:38-0700 (2s ago)\n",
      "2016-07-08 03:41:40.830528 : process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:40.830538 : mark_done process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:41:40.830548 : ##############################################################################\n",
      "2016-07-08 03:41:40.830557 \n",
      "2016-07-08 03:41:40.830568 : ## data/sentences ############################################################\n",
      "2016-07-08 03:41:40.830578 : # Done: 2016-07-08T03:41:38-0700 (2s ago)\n",
      "2016-07-08 03:41:40.830588 : # no-op\n",
      "2016-07-08 03:41:40.830597 : mark_done data/sentences\n",
      "2016-07-08 03:41:40.830607 : ##############################################################################\n",
      "2016-07-08 03:41:40.830616 \n",
      "2016-07-08 03:41:40.830625 ## process/ext_person_mention_by_map_person_mention ##########################\n",
      "2016-07-08 03:41:40.830634 # Done: N/A\n",
      "2016-07-08 03:41:40.830644 process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:41:40.830653 ++ dirname process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:41:40.830663 + cd process/ext_person_mention_by_map_person_mention\n",
      "2016-07-08 03:41:40.830672 + : dd_tmp_ dd_old_\n",
      "2016-07-08 03:41:40.830681 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_person_mention_by_map_person_mention\n",
      "2016-07-08 03:41:40.830691 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_person_mention_by_map_person_mention\n",
      "2016-07-08 03:41:40.830700 + export DEEPDIVE_LOAD_FORMAT=tsj\n",
      "2016-07-08 03:41:40.830709 + DEEPDIVE_LOAD_FORMAT=tsj\n",
      "2016-07-08 03:41:40.830719 + output_relation=person_mention\n",
      "2016-07-08 03:41:40.830728 + output_relation_tmp=dd_tmp_person_mention\n",
      "2016-07-08 03:41:40.830738 + output_relation_old=dd_old_person_mention\n",
      "2016-07-08 03:41:40.830747 + deepdive create table-if-not-exists person_mention\n",
      "2016-07-08 03:41:41.020315 CREATE TABLE\n",
      "2016-07-08 03:41:41.021162 + deepdive create table dd_tmp_person_mention like person_mention\n",
      "2016-07-08 03:41:41.228540 CREATE TABLE\n",
      "2016-07-08 03:41:41.229545 + deepdive compute execute 'input_sql=\n",
      "2016-07-08 03:41:41.229592 \n",
      "2016-07-08 03:41:41.229615 SELECT R0.doc_id AS column_0\n",
      "2016-07-08 03:41:41.229647      , R0.sentence_index AS column_1\n",
      "2016-07-08 03:41:41.229683      , R0.tokens AS column_2\n",
      "2016-07-08 03:41:41.229703      , R0.ner_tags AS column_3\n",
      "2016-07-08 03:41:41.229715 FROM sentences R0\n",
      "2016-07-08 03:41:41.229727 \n",
      "2016-07-08 03:41:41.229738 ' 'command=cd \"$DEEPDIVE_APP\" && udf/map_person_mention.py' output_relation=dd_tmp_person_mention\n",
      "2016-07-08 03:41:41.398458 Executing with the following configuration:\n",
      "2016-07-08 03:41:41.398513  DEEPDIVE_NUM_PROCESSES=7\n",
      "2016-07-08 03:41:41.398529  DEEPDIVE_NUM_PARALLEL_UNLOADS=1\n",
      "2016-07-08 03:41:41.398541  DEEPDIVE_NUM_PARALLEL_LOADS=1\n",
      "2016-07-08 03:41:41.541370 unloading to feed_processes-1: '\n",
      "2016-07-08 03:41:41.541426 \n",
      "2016-07-08 03:41:41.541448 SELECT R0.doc_id AS column_0\n",
      "2016-07-08 03:41:41.541469      , R0.sentence_index AS column_1\n",
      "2016-07-08 03:41:41.541487      , R0.tokens AS column_2\n",
      "2016-07-08 03:41:41.541506      , R0.ner_tags AS column_3\n",
      "2016-07-08 03:41:41.541522 FROM sentences R0\n",
      "2016-07-08 03:41:41.541542 \n",
      "2016-07-08 03:41:41.541560 '\n",
      "2016-07-08 03:41:41.777562 Loading dd_tmp_person_mention from output_computed-1 (tsj format)\n",
      "2016-07-08 03:41:41.917691 COPY 1760\n",
      "2016-07-08 03:41:41.922828 + : 'Replacing person_mention with dd_tmp_person_mention'\n",
      "2016-07-08 03:41:41.922868 + deepdive sql 'DROP TABLE IF EXISTS dd_old_person_mention CASCADE;'\n",
      "2016-07-08 03:41:41.996301 DROP TABLE\n",
      "2016-07-08 03:41:41.997142 + deepdive sql 'ALTER TABLE person_mention     RENAME TO dd_old_person_mention;'\n",
      "2016-07-08 03:41:42.070807 ALTER TABLE\n",
      "2016-07-08 03:41:42.071881 + deepdive sql 'ALTER TABLE dd_tmp_person_mention RENAME TO person_mention;'\n",
      "2016-07-08 03:41:42.145907 ALTER TABLE\n",
      "2016-07-08 03:41:42.146739 + deepdive sql 'DROP TABLE IF EXISTS dd_old_person_mention CASCADE;'\n",
      "2016-07-08 03:41:42.222048 DROP TABLE\n",
      "2016-07-08 03:41:42.222673 + deepdive db analyze person_mention\n",
      "2016-07-08 03:41:42.349348 ANALYZE\n",
      "2016-07-08 03:41:42.350801 mark_done process/ext_person_mention_by_map_person_mention\n",
      "2016-07-08 03:41:42.364340 ##############################################################################\n",
      "2016-07-08 03:41:42.364390 \n",
      "2016-07-08 03:41:42.364410 ## data/person_mention #######################################################\n",
      "2016-07-08 03:41:42.364426 # Done: 2016-07-08T03:39:13-0700 (2m 27s ago)\n",
      "2016-07-08 03:41:42.364442 # no-op\n",
      "2016-07-08 03:41:42.364457 mark_done data/person_mention\n",
      "2016-07-08 03:41:42.376561 ##############################################################################\n",
      "2016-07-08 03:41:42.376629 \n",
      "���run/FINISHED��� -> ���run/FINISHED~���\n",
      "���run/FINISHED��� -> ���20160708/034140.582441000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo person_mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name        |                 doc                  | sentence | begin | end \n",
      "--------------------+--------------------------------------+----------+-------+-----\n",
      " Betty Estenson     | 15d53efb-2151-4164-aee0-cae51faedeeb |        1 |    19 |  20\n",
      " Edwin              | 15d53efb-2151-4164-aee0-cae51faedeeb |        1 |    24 |  24\n",
      " Dame Joan Collins  | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |        0 |     0 |   2\n",
      " Jackie             | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |        0 |    12 |  12\n",
      " LA Jackie          | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |       17 |    17 |  18\n",
      " Sugar Ray Robinson | ba44d0cd-bff2-4875-8036-86f37419b5e7 |       43 |     3 |   5\n",
      " Ms Credlin         | c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325 |       24 |     5 |   6\n",
      " Alex Ellinghausen  | c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325 |       13 |     2 |   3\n",
      " James Massola      | c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325 |       33 |     1 |   2\n",
      " Paul               | ba44d0cd-bff2-4875-8036-86f37419b5e7 |       47 |     0 |   0\n",
      " Alex               | 0d07e617-00d4-4866-aee2-0ae197ae366f |       21 |     1 |   1\n",
      " Alex               | 0d07e617-00d4-4866-aee2-0ae197ae366f |       22 |     1 |   1\n",
      " George Eliot       | 43c02bac-d556-4851-9b04-7773700759b6 |       17 |    18 |  19\n",
      " Wordsworth         | 43c02bac-d556-4851-9b04-7773700759b6 |       29 |     2 |   2\n",
      " Stephen            | 43c02bac-d556-4851-9b04-7773700759b6 |       29 |     6 |   6\n",
      " Wordsworth         | 43c02bac-d556-4851-9b04-7773700759b6 |       29 |    27 |  27\n",
      " Andrade            | 8bd53371-3b63-4bb0-acc7-5e2c2ecc8ff4 |        4 |     3 |   3\n",
      " Swindell           | 8bd53371-3b63-4bb0-acc7-5e2c2ecc8ff4 |        5 |     1 |   1\n",
      " Swindell           | 8bd53371-3b63-4bb0-acc7-5e2c2ecc8ff4 |        6 |     1 |   1\n",
      " Andrade            | 8bd53371-3b63-4bb0-acc7-5e2c2ecc8ff4 |        7 |     1 |   1\n",
      "(20 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "deepdive query '\n",
    "    name, doc, sentence, begin, end | 20\n",
    "    ?- person_mention(p_id, name, doc, sentence, begin, end).\n",
    "'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions of spouses (pairs of people)\n",
    "Next, we'll take all pairs of **non-overlapping person mentions that co-occur in a sentence with less than 5 people total,** and consider these as the set of potential ('candidate') spouse mentions.\n",
    "We thus filter out sentences with large numbers of people for the purposes of this tutorial; however, these could be included if desired.\n",
    "Again, to start, we declare the schema for our `spouse_candidate` table—here just the two names, and the two `person_mention` IDs referred to:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "spouse_candidate(\n",
    "    p1_id   text,\n",
    "    p1_name text,\n",
    "    p2_id   text,\n",
    "    p2_name text\n",
    ").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for this operation we don't use any UDF script, instead rely entirely on DDlog operations.\n",
    "We simply construct a table of person counts, and then do a join with our filtering conditions.\n",
    "In DDlog this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "num_people(doc_id, sentence_index, COUNT(p)) :-\n",
    "    person_mention(p, _, doc_id, sentence_index, _, _).\n",
    "\n",
    "spouse_candidate(p1, p1_name, p2, p2_name) :-\n",
    "    num_people(same_doc, same_sentence, num_p),\n",
    "    person_mention(p1, p1_name, same_doc, same_sentence, p1_begin, _),\n",
    "    person_mention(p2, p2_name, same_doc, same_sentence, p2_begin, _),\n",
    "    num_p < 5,\n",
    "    p1 < p2,\n",
    "    p1_name != p2_name,\n",
    "    p1_begin != p2_begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tell DeepDive to run what we have so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mapp.ddlog: updated since last `deepdive compile`\n",
      "\u001b[0m2016-07-08 03:41:43.236742 ���run/LATEST.COMPILE��� -> ���20160708/034143.194412000���\n",
      "2016-07-08 03:41:43.236940 ���run/RUNNING.COMPILE��� -> ���20160708/034143.194412000���\n",
      "2016-07-08 03:41:43.236969 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 03:41:43.236991  run/compiled/deepdive.conf\n",
      "2016-07-08 03:41:43.237010   from app.ddlog\n",
      "2016-07-08 03:41:43.704620  run/compiled/deepdive.conf.json\n",
      "2016-07-08 03:41:43.848082 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 03:41:43.876719  checking if input_extractors_well_defined\n",
      "2016-07-08 03:41:43.876782  checking if input_schema_wellformed\n",
      "2016-07-08 03:41:43.877126 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 03:41:43.877512  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 03:41:43.891542  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 03:41:43.904211  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 03:41:43.922053  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 03:41:43.935076  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 03:41:43.948985  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 03:41:43.965902  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 03:41:43.988035  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 03:41:44.065632  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 03:41:44.101803  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 03:41:44.117543  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 03:41:44.132789  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 03:41:44.147656  run/compiled/config.json\n",
      "2016-07-08 03:41:44.151129 Validating run/compiled/config.json:\n",
      "2016-07-08 03:41:44.182048  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 03:41:44.182097  checking if compiled_dependencies_correct\n",
      "2016-07-08 03:41:44.182110  checking if compiled_input_output_well_defined\n",
      "2016-07-08 03:41:44.182121  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 03:41:44.182355 Compiling executable code into:\n",
      "2016-07-08 03:41:44.182634  run/compiled/code-Makefile.json\n",
      "2016-07-08 03:41:44.183058  run/compiled/code-cmd_extractor.json\n",
      "2016-07-08 03:41:44.183410  run/compiled/code-dataflow_dot.json\n",
      "2016-07-08 03:41:44.183790  run/compiled/code-sql_extractor.json\n",
      "2016-07-08 03:41:44.184163  run/compiled/code-tsX_extractor.json\n",
      "2016-07-08 03:41:44.217679 Validating run/compiled/code-*.json:\n",
      "2016-07-08 03:41:44.242177  checking if codegen_no_path_collision\n",
      "2016-07-08 03:41:44.245177 Generating files:\n",
      "2016-07-08 03:41:44.260263  run/Makefile\n",
      "2016-07-08 03:41:44.260673  run/process/init/app/run.sh\n",
      "2016-07-08 03:41:44.261064  run/process/init/relation/has_spouse/run.sh\n",
      "2016-07-08 03:41:44.261476  run/process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:44.262250  run/dataflow.dot\n",
      "2016-07-08 03:41:44.262582  run/process/ext_num_people/run.sh\n",
      "2016-07-08 03:41:44.263387  run/process/ext_spouse_candidate/run.sh\n",
      "2016-07-08 03:41:44.264677  run/process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:44.265615  run/process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:41:44.312486  run/dataflow.pdf\n",
      "2016-07-08 03:41:44.313366   (file:///ConfinedWater/spouse/run/dataflow.pdf)\n",
      "2016-07-08 03:41:44.334437  run/dataflow.svg\n",
      "2016-07-08 03:41:44.335363   (file:///ConfinedWater/spouse/run/dataflow.svg)\n",
      "2016-07-08 03:41:44.554350 ���run/compiled��� -> ���run/compiled~���\n",
      "2016-07-08 03:41:44.556916 ���run/compiled��� -> ���20160708/034143.194412000���\n",
      "���data/spouse_candidate.done��� -> ���data/spouse_candidate.done~���\n",
      "���process/ext_spouse_candidate.done��� -> ���process/ext_spouse_candidate.done~���\n",
      "���run/RUNNING��� -> ���20160708/034144.766070000���\n",
      "���run/LATEST��� -> ���20160708/034144.766070000���\n",
      "2016-07-08 03:41:45.146900 #!/bin/sh\n",
      "2016-07-08 03:41:45.147063 # Host: scuba.local\n",
      "2016-07-08 03:41:45.147082 # App: /ConfinedWater/spouse\n",
      "2016-07-08 03:41:45.147094 # Targets: spouse_candidate\n",
      "2016-07-08 03:41:45.147104 # Plan: run/20160708/034144.766070000/plan.sh\n",
      "2016-07-08 03:41:45.147115 export DEEPDIVE_PWD='/ConfinedWater/spouse'\n",
      "2016-07-08 03:41:45.147125 # execution plan for data/spouse_candidate\n",
      "2016-07-08 03:41:45.147134 \n",
      "2016-07-08 03:41:45.147145 : ## process/init/app ##########################################################\n",
      "2016-07-08 03:41:45.147155 : # Done: 2016-07-08T03:41:10-0700 (34s ago)\n",
      "2016-07-08 03:41:45.147165 : process/init/app/run.sh\n",
      "2016-07-08 03:41:45.147174 : mark_done process/init/app\n",
      "2016-07-08 03:41:45.147184 : ##############################################################################\n",
      "2016-07-08 03:41:45.147194 \n",
      "2016-07-08 03:41:45.147204 : ## process/init/relation/articles ############################################\n",
      "2016-07-08 03:41:45.147213 : # Done: 2016-07-08T03:41:15-0700 (29s ago)\n",
      "2016-07-08 03:41:45.147223 : process/init/relation/articles/run.sh\n",
      "2016-07-08 03:41:45.147232 : mark_done process/init/relation/articles\n",
      "2016-07-08 03:41:45.147242 : ##############################################################################\n",
      "2016-07-08 03:41:45.147253 \n",
      "2016-07-08 03:41:45.147262 : ## data/articles #############################################################\n",
      "2016-07-08 03:41:45.147271 : # Done: 2016-07-08T03:41:15-0700 (29s ago)\n",
      "2016-07-08 03:41:45.147281 : # no-op\n",
      "2016-07-08 03:41:45.147290 : mark_done data/articles\n",
      "2016-07-08 03:41:45.147299 : ##############################################################################\n",
      "2016-07-08 03:41:45.147308 \n",
      "2016-07-08 03:41:45.147318 : ## process/ext_sentences_by_nlp_markup #######################################\n",
      "2016-07-08 03:41:45.147328 : # Done: 2016-07-08T03:41:38-0700 (6s ago)\n",
      "2016-07-08 03:41:45.147337 : process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:41:45.147346 : mark_done process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:41:45.147356 : ##############################################################################\n",
      "2016-07-08 03:41:45.147366 \n",
      "2016-07-08 03:41:45.147376 : ## data/sentences ############################################################\n",
      "2016-07-08 03:41:45.147385 : # Done: 2016-07-08T03:41:38-0700 (6s ago)\n",
      "2016-07-08 03:41:45.147394 : # no-op\n",
      "2016-07-08 03:41:45.147403 : mark_done data/sentences\n",
      "2016-07-08 03:41:45.147413 : ##############################################################################\n",
      "2016-07-08 03:41:45.147422 \n",
      "2016-07-08 03:41:45.147431 : ## process/ext_person_mention_by_map_person_mention ##########################\n",
      "2016-07-08 03:41:45.147441 : # Done: 2016-07-08T03:41:42-0700 (2s ago)\n",
      "2016-07-08 03:41:45.147450 : process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:41:45.147459 : mark_done process/ext_person_mention_by_map_person_mention\n",
      "2016-07-08 03:41:45.147468 : ##############################################################################\n",
      "2016-07-08 03:41:45.147477 \n",
      "2016-07-08 03:41:45.147487 : ## data/person_mention #######################################################\n",
      "2016-07-08 03:41:45.147496 : # Done: 2016-07-08T03:41:42-0700 (3s ago)\n",
      "2016-07-08 03:41:45.147505 : # no-op\n",
      "2016-07-08 03:41:45.147515 : mark_done data/person_mention\n",
      "2016-07-08 03:41:45.147525 : ##############################################################################\n",
      "2016-07-08 03:41:45.147534 \n",
      "2016-07-08 03:41:45.147543 ## process/ext_num_people ####################################################\n",
      "2016-07-08 03:41:45.147553 : # Done: 2016-07-08T03:40:33-0700 (1m 12s ago)\n",
      "2016-07-08 03:41:45.147562 # Done: 2016-07-08T03:40:33-0700 (1m 11s ago)\n",
      "2016-07-08 03:41:45.147571 process/ext_num_people/run.sh\n",
      "2016-07-08 03:41:45.147581 ++ dirname process/ext_num_people/run.sh\n",
      "2016-07-08 03:41:45.147590 + cd process/ext_num_people\n",
      "2016-07-08 03:41:45.147599 + : dd_tmp_ dd_old_\n",
      "2016-07-08 03:41:45.147608 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_num_people\n",
      "2016-07-08 03:41:45.147618 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_num_people\n",
      "2016-07-08 03:41:45.147627 + deepdive create view num_people as '\n",
      "2016-07-08 03:41:45.147636 SELECT \"R\".\"column_0\" AS \"column_0\"\n",
      "2016-07-08 03:41:45.147645      , \"R\".\"column_1\" AS \"column_1\"\n",
      "2016-07-08 03:41:45.147654      , \"R\".\"column_2\" AS \"column_2\"\n",
      "2016-07-08 03:41:45.147664 FROM (\n",
      "2016-07-08 03:41:45.147673 SELECT R0.doc_id AS column_0\n",
      "2016-07-08 03:41:45.147682      , R0.sentence_index AS column_1\n",
      "2016-07-08 03:41:45.147692      , COUNT(R0.mention_id) AS column_2\n",
      "2016-07-08 03:41:45.147701 FROM person_mention R0\n",
      "2016-07-08 03:41:45.147710 GROUP BY R0.doc_id\n",
      "2016-07-08 03:41:45.147720     , R0.sentence_index\n",
      "2016-07-08 03:41:45.147729 ) \"R\"\n",
      "2016-07-08 03:41:45.147738 '\n",
      "2016-07-08 03:41:45.227146 CREATE VIEW\n",
      "2016-07-08 03:41:45.228297 mark_done process/ext_num_people\n",
      "2016-07-08 03:41:45.240439 ##############################################################################\n",
      "2016-07-08 03:41:45.240477 \n",
      "2016-07-08 03:41:45.240491 ## data/num_people ###########################################################\n",
      "2016-07-08 03:41:45.240503 : # Done: 2016-07-08T03:40:33-0700 (1m 12s ago)\n",
      "2016-07-08 03:41:45.240514 # Done: 2016-07-08T03:40:33-0700 (1m 11s ago)\n",
      "2016-07-08 03:41:45.240525 # no-op\n",
      "2016-07-08 03:41:45.240535 mark_done data/num_people\n",
      "2016-07-08 03:41:45.251748 ##############################################################################\n",
      "2016-07-08 03:41:45.251798 \n",
      "2016-07-08 03:41:45.251823 ## process/ext_spouse_candidate ##############################################\n",
      "2016-07-08 03:41:45.251843 : # Done: 2016-07-08T03:40:33-0700 (1m 12s ago)\n",
      "2016-07-08 03:41:45.251862 # Done: 2016-07-08T03:40:33-0700 (1m 11s ago)\n",
      "2016-07-08 03:41:45.251880 process/ext_spouse_candidate/run.sh\n",
      "2016-07-08 03:41:45.256824 ++ dirname process/ext_spouse_candidate/run.sh\n",
      "2016-07-08 03:41:45.258752 + cd process/ext_spouse_candidate\n",
      "2016-07-08 03:41:45.258827 + : dd_tmp_ dd_old_\n",
      "2016-07-08 03:41:45.258847 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_spouse_candidate\n",
      "2016-07-08 03:41:45.258859 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_spouse_candidate\n",
      "2016-07-08 03:41:45.258915 + deepdive create view spouse_candidate as '\n",
      "2016-07-08 03:41:45.258930 SELECT \"R\".\"column_0\" AS \"p1_id\"\n",
      "2016-07-08 03:41:45.258940      , \"R\".\"column_1\" AS \"p1_name\"\n",
      "2016-07-08 03:41:45.258950      , \"R\".\"column_2\" AS \"p2_id\"\n",
      "2016-07-08 03:41:45.258959      , \"R\".\"column_3\" AS \"p2_name\"\n",
      "2016-07-08 03:41:45.258970 FROM (\n",
      "2016-07-08 03:41:45.258979 SELECT R1.mention_id AS column_0\n",
      "2016-07-08 03:41:45.258989      , R1.mention_text AS column_1\n",
      "2016-07-08 03:41:45.258999      , R2.mention_id AS column_2\n",
      "2016-07-08 03:41:45.259008      , R2.mention_text AS column_3\n",
      "2016-07-08 03:41:45.259018 FROM num_people R0\n",
      "2016-07-08 03:41:45.259029    , person_mention R1\n",
      "2016-07-08 03:41:45.259038    , person_mention R2\n",
      "2016-07-08 03:41:45.259048 WHERE R1.doc_id = R0.column_0\n",
      "2016-07-08 03:41:45.259058   AND R1.sentence_index = R0.column_1\n",
      "2016-07-08 03:41:45.259068   AND R2.doc_id = R0.column_0\n",
      "2016-07-08 03:41:45.259078   AND R2.sentence_index = R0.column_1\n",
      "2016-07-08 03:41:45.259088   AND R0.column_2 < 5\n",
      "2016-07-08 03:41:45.259098   AND R1.mention_id < R2.mention_id\n",
      "2016-07-08 03:41:45.259108   AND R1.mention_text != R2.mention_text\n",
      "2016-07-08 03:41:45.259118   AND R1.begin_index != R2.begin_index\n",
      "2016-07-08 03:41:45.259128 ) \"R\"\n",
      "2016-07-08 03:41:45.259138 '\n",
      "2016-07-08 03:41:45.357941 CREATE VIEW\n",
      "2016-07-08 03:41:45.359168 mark_done process/ext_spouse_candidate\n",
      "2016-07-08 03:41:45.372204 ##############################################################################\n",
      "2016-07-08 03:41:45.372254 \n",
      "2016-07-08 03:41:45.372268 ## data/spouse_candidate #####################################################\n",
      "2016-07-08 03:41:45.372279 : # Done: 2016-07-08T03:40:33-0700 (1m 12s ago)\n",
      "2016-07-08 03:41:45.372291 # Done: 2016-07-08T03:40:33-0700 (1m 11s ago)\n",
      "2016-07-08 03:41:45.372303 # no-op\n",
      "2016-07-08 03:41:45.372327 mark_done data/spouse_candidate\n",
      "2016-07-08 03:41:45.383962 ##############################################################################\n",
      "2016-07-08 03:41:45.384036 \n",
      "���run/FINISHED��� -> ���run/FINISHED~���\n",
      "���run/FINISHED��� -> ���20160708/034144.766070000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo spouse_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name1       |     name2     |                 doc                  | sentence \n",
      "-------------------+---------------+--------------------------------------+----------\n",
      " Betty Estenson    | Edwin         | 15d53efb-2151-4164-aee0-cae51faedeeb |        1\n",
      " Dame Joan Collins | Jackie        | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |        0\n",
      " Wordsworth        | Stephen       | 43c02bac-d556-4851-9b04-7773700759b6 |       29\n",
      " Wordsworth        | Stephen       | 43c02bac-d556-4851-9b04-7773700759b6 |       29\n",
      " Rogers            | Ginny         | c27a162d-f2d1-4bdb-84ba-0915a082775b |       19\n",
      " Johnson           | Rogers        | c27a162d-f2d1-4bdb-84ba-0915a082775b |       40\n",
      " Irene             | John          | 5dd9bf47-c8a9-49e3-8d02-994f8eabb91a |        1\n",
      " John              | Mary Jacobsky | 5dd9bf47-c8a9-49e3-8d02-994f8eabb91a |        1\n",
      " Irene             | Mary Jacobsky | 5dd9bf47-c8a9-49e3-8d02-994f8eabb91a |        1\n",
      " Dame Joan         | Jackie        | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |       13\n",
      " Peter Holm        | Jackie        | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |       14\n",
      " Dame Joan         | Jackie        | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |       14\n",
      " Dame Joan         | Peter Holm    | df13cc43-53fd-4f09-9a7e-d69b12a4adc0 |       14\n",
      " Khoury            | Greg Medcraft | ebcd41ea-e5b4-43a4-9e16-4406d81cfcda |       34\n",
      " Howard            | Sinodinos     | c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325 |       30\n",
      " Howard            | Turnbull      | c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325 |       30\n",
      " Sinodinos         | Turnbull      | c5f8a528-cc0f-4f3e-aaef-b9e3b6b00325 |       30\n",
      " Sherlock Holmes   | Sherlock      | 8b31ede3-0f3b-431a-86a3-342ee18cfd83 |       17\n",
      " Alex              | Liz           | 0d07e617-00d4-4866-aee2-0ae197ae366f |       19\n",
      " Jay Scott         | Liz           | 0d07e617-00d4-4866-aee2-0ae197ae366f |       19\n",
      "(20 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "deepdive query '\n",
    "    name1, name2, doc, sentence | 20\n",
    "    ?- spouse_candidate(p1, name1, p2, name2),\n",
    "       person_mention(p1, _, doc, sentence, _, _).\n",
    "'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Extracting features for each candidate\n",
    "Finally, we will extract a set of **features** for each candidate:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "## Feature Extraction #########################################################\n",
    " \n",
    "# Feature extraction (using DDLIB via a UDF) at the relation level\n",
    "spouse_feature(\n",
    "    p1_id   text,\n",
    "    p2_id   text,\n",
    "    feature text\n",
    ").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to represent each spouse candidate mention by a set of attributes or **_features_** which capture at least the key aspects of the mention, and then let a machine learning model learn how much each feature is correlated with our decision variable ('is this a spouse mention?').\n",
    "For those who have worked with machine learning systems before, note that we are using a sparse storage representation-\n",
    "you could think of a spouse candidate `(p1_id, p2_id)` as being represented by a vector of length `L = COUNT(DISTINCT feature)`, consisting of all zeros except for at the indexes specified by the rows with key `(p1_id, p2_id)`.\n",
    "\n",
    "DeepDive includes an [automatic feature generation library, DDlib](http://deepdive.stanford.edu/gen_feats), which we will use here.\n",
    "Although many state-of-the-art [applications](http://deepdive.stanford.edu/showcase/apps) have been built using purely DDlib-generated features, others can be used and/or added as well.\n",
    "To use DDlib, we create a list of `ddlib.Word` objects, two `ddlib.Span` objects, and then use the function `get_generic_features_relation`, as shown in the following Python code for `udf/extract_spouse_features.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing udf/extract_spouse_features.py\n"
     ]
    }
   ],
   "source": [
    "%%file udf/extract_spouse_features.py\n",
    "#!/usr/bin/env python\n",
    "from deepdive import *\n",
    "import ddlib\n",
    "\n",
    "@tsj_extractor\n",
    "@returns(lambda\n",
    "        p1_id   = \"text\",\n",
    "        p2_id   = \"text\",\n",
    "        feature = \"text\",\n",
    "    :[])\n",
    "def extract(\n",
    "        p1_id          = \"text\",\n",
    "        p2_id          = \"text\",\n",
    "        p1_begin_index = \"int\",\n",
    "        p1_end_index   = \"int\",\n",
    "        p2_begin_index = \"int\",\n",
    "        p2_end_index   = \"int\",\n",
    "        doc_id         = \"text\",\n",
    "        sent_index     = \"int\",\n",
    "        tokens         = \"text[]\",\n",
    "        lemmas         = \"text[]\",\n",
    "        pos_tags       = \"text[]\",\n",
    "        ner_tags       = \"text[]\",\n",
    "        dep_types      = \"text[]\",\n",
    "        dep_parents    = \"int[]\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Uses DDLIB to generate features for the spouse relation.\n",
    "    \"\"\"\n",
    "    # Create a DDLIB sentence object, which is just a list of DDLIB Word objects\n",
    "    sent = []\n",
    "    for i,t in enumerate(tokens):\n",
    "        sent.append(ddlib.Word(\n",
    "            begin_char_offset=None,\n",
    "            end_char_offset=None,\n",
    "            word=t,\n",
    "            lemma=lemmas[i],\n",
    "            pos=pos_tags[i],\n",
    "            ner=ner_tags[i],\n",
    "            dep_par=dep_parents[i] - 1,  # Note that as stored from CoreNLP 0 is ROOT, but for DDLIB -1 is ROOT\n",
    "            dep_label=dep_types[i]))\n",
    "\n",
    "    # Create DDLIB Spans for the two person mentions\n",
    "    p1_span = ddlib.Span(begin_word_id=p1_begin_index, length=(p1_end_index-p1_begin_index+1))\n",
    "    p2_span = ddlib.Span(begin_word_id=p2_begin_index, length=(p2_end_index-p2_begin_index+1))\n",
    "\n",
    "    # Generate the generic features using DDLIB\n",
    "    for feature in ddlib.get_generic_features_relation(sent, p1_span, p2_span):\n",
    "        yield [p1_id, p2_id, feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x udf/extract_spouse_features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that getting the input for this UDF requires joining the `person_mention` and `sentences` tables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "function extract_spouse_features over (\n",
    "        p1_id          text,\n",
    "        p2_id          text,\n",
    "        p1_begin_index int,\n",
    "        p1_end_index   int,\n",
    "        p2_begin_index int,\n",
    "        p2_end_index   int,\n",
    "        doc_id         text,\n",
    "        sent_index     int,\n",
    "        tokens         text[],\n",
    "        lemmas         text[],\n",
    "        pos_tags       text[],\n",
    "        ner_tags       text[],\n",
    "        dep_types      text[],\n",
    "        dep_tokens     int[]\n",
    "    ) returns rows like spouse_feature\n",
    "    implementation \"udf/extract_spouse_features.py\" handles tsj lines.\n",
    "\n",
    "spouse_feature += extract_spouse_features(\n",
    "    p1_id, p2_id, p1_begin_index, p1_end_index, p2_begin_index, p2_end_index,\n",
    "    doc_id, sent_index, tokens, lemmas, pos_tags, ner_tags, dep_types, dep_tokens\n",
    ") :-\n",
    "    person_mention(p1_id, _, doc_id, sent_index, p1_begin_index, p1_end_index),\n",
    "    person_mention(p2_id, _, doc_id, sent_index, p2_begin_index, p2_end_index),\n",
    "    sentences(doc_id, sent_index, tokens, lemmas, pos_tags, ner_tags, _, dep_types, dep_tokens).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute this UDF to get our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mapp.ddlog: updated since last `deepdive compile`\n",
      "\u001b[0m2016-07-08 03:45:44.659009 ���run/LATEST.COMPILE��� -> ���20160708/034544.599042000���\n",
      "2016-07-08 03:45:44.659160 ���run/RUNNING.COMPILE��� -> ���20160708/034544.599042000���\n",
      "2016-07-08 03:45:44.659187 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 03:45:44.659200  run/compiled/deepdive.conf\n",
      "2016-07-08 03:45:44.659212   from app.ddlog\n",
      "2016-07-08 03:45:45.281144  run/compiled/deepdive.conf.json\n",
      "2016-07-08 03:45:45.490482 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 03:45:45.522665  checking if input_extractors_well_defined\n",
      "2016-07-08 03:45:45.522727  checking if input_schema_wellformed\n",
      "2016-07-08 03:45:45.523003 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 03:45:45.523623  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 03:45:45.538485  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 03:45:45.552213  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 03:45:45.570359  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 03:45:45.583260  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 03:45:45.597708  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 03:45:45.613722  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 03:45:45.635537  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 03:45:45.709082  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 03:45:45.743219  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 03:45:45.759138  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 03:45:45.773514  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 03:45:45.788404  run/compiled/config.json\n",
      "2016-07-08 03:45:45.790990 Validating run/compiled/config.json:\n",
      "2016-07-08 03:45:45.821500  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 03:45:45.821539  checking if compiled_dependencies_correct\n",
      "2016-07-08 03:45:45.821554  checking if compiled_input_output_well_defined\n",
      "2016-07-08 03:45:45.821566  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 03:45:45.821841 Compiling executable code into:\n",
      "2016-07-08 03:45:45.822367  run/compiled/code-Makefile.json\n",
      "2016-07-08 03:45:45.822708  run/compiled/code-cmd_extractor.json\n",
      "2016-07-08 03:45:45.823065  run/compiled/code-dataflow_dot.json\n",
      "2016-07-08 03:45:45.823454  run/compiled/code-sql_extractor.json\n",
      "2016-07-08 03:45:45.823768  run/compiled/code-tsX_extractor.json\n",
      "2016-07-08 03:45:45.857587 Validating run/compiled/code-*.json:\n",
      "2016-07-08 03:45:45.883226  checking if codegen_no_path_collision\n",
      "2016-07-08 03:45:45.887598 Generating files:\n",
      "2016-07-08 03:45:45.904620  run/Makefile\n",
      "2016-07-08 03:45:45.904972  run/process/init/app/run.sh\n",
      "2016-07-08 03:45:45.905339  run/process/init/relation/has_spouse/run.sh\n",
      "2016-07-08 03:45:45.905653  run/process/init/relation/articles/run.sh\n",
      "2016-07-08 03:45:45.906422  run/dataflow.dot\n",
      "2016-07-08 03:45:45.906751  run/process/ext_num_people/run.sh\n",
      "2016-07-08 03:45:45.907545  run/process/ext_spouse_candidate/run.sh\n",
      "2016-07-08 03:45:45.908598  run/process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:45:45.909506  run/process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:45:45.911517  run/process/ext_spouse_feature_by_extract_spouse_features/run.sh\n",
      "2016-07-08 03:45:45.979590  run/dataflow.pdf\n",
      "2016-07-08 03:45:45.980430   (file:///ConfinedWater/spouse/run/dataflow.pdf)\n",
      "2016-07-08 03:45:46.001589  run/dataflow.svg\n",
      "2016-07-08 03:45:46.002480   (file:///ConfinedWater/spouse/run/dataflow.svg)\n",
      "2016-07-08 03:45:46.233650 ���run/compiled��� -> ���run/compiled~���\n",
      "2016-07-08 03:45:46.236277 ���run/compiled��� -> ���20160708/034544.599042000���\n",
      "���run/RUNNING��� -> ���20160708/034546.442183000���\n",
      "���run/LATEST��� -> ���20160708/034546.442183000���\n",
      "2016-07-08 03:45:46.742161 #!/bin/sh\n",
      "2016-07-08 03:45:46.742358 # Host: scuba.local\n",
      "2016-07-08 03:45:46.742377 # App: /ConfinedWater/spouse\n",
      "2016-07-08 03:45:46.742391 # Targets: spouse_feature\n",
      "2016-07-08 03:45:46.742407 # Plan: run/20160708/034546.442183000/plan.sh\n",
      "2016-07-08 03:45:46.742424 export DEEPDIVE_PWD='/ConfinedWater/spouse'\n",
      "2016-07-08 03:45:46.742442 # execution plan for data/spouse_feature\n",
      "2016-07-08 03:45:46.742458 \n",
      "2016-07-08 03:45:46.742470 : ## process/init/app ##########################################################\n",
      "2016-07-08 03:45:46.742480 : # Done: 2016-07-08T03:41:10-0700 (4m 36s ago)\n",
      "2016-07-08 03:45:46.742490 : process/init/app/run.sh\n",
      "2016-07-08 03:45:46.742499 : mark_done process/init/app\n",
      "2016-07-08 03:45:46.742508 : ##############################################################################\n",
      "2016-07-08 03:45:46.742517 \n",
      "2016-07-08 03:45:46.742527 : ## process/init/relation/articles ############################################\n",
      "2016-07-08 03:45:46.742536 : # Done: 2016-07-08T03:41:15-0700 (4m 31s ago)\n",
      "2016-07-08 03:45:46.742545 : process/init/relation/articles/run.sh\n",
      "2016-07-08 03:45:46.742556 : mark_done process/init/relation/articles\n",
      "2016-07-08 03:45:46.742573 : ##############################################################################\n",
      "2016-07-08 03:45:46.742588 \n",
      "2016-07-08 03:45:46.742602 : ## data/articles #############################################################\n",
      "2016-07-08 03:45:46.742616 : # Done: 2016-07-08T03:41:15-0700 (4m 31s ago)\n",
      "2016-07-08 03:45:46.742626 : # no-op\n",
      "2016-07-08 03:45:46.742635 : mark_done data/articles\n",
      "2016-07-08 03:45:46.742644 : ##############################################################################\n",
      "2016-07-08 03:45:46.742653 \n",
      "2016-07-08 03:45:46.742663 : ## process/ext_sentences_by_nlp_markup #######################################\n",
      "2016-07-08 03:45:46.742672 : # Done: 2016-07-08T03:41:38-0700 (4m 8s ago)\n",
      "2016-07-08 03:45:46.742681 : process/ext_sentences_by_nlp_markup/run.sh\n",
      "2016-07-08 03:45:46.742690 : mark_done process/ext_sentences_by_nlp_markup\n",
      "2016-07-08 03:45:46.742700 : ##############################################################################\n",
      "2016-07-08 03:45:46.742710 \n",
      "2016-07-08 03:45:46.742720 : ## data/sentences ############################################################\n",
      "2016-07-08 03:45:46.742736 : # Done: 2016-07-08T03:41:38-0700 (4m 8s ago)\n",
      "2016-07-08 03:45:46.742749 : # no-op\n",
      "2016-07-08 03:45:46.742762 : mark_done data/sentences\n",
      "2016-07-08 03:45:46.742774 : ##############################################################################\n",
      "2016-07-08 03:45:46.742783 \n",
      "2016-07-08 03:45:46.742793 : ## process/ext_person_mention_by_map_person_mention ##########################\n",
      "2016-07-08 03:45:46.742802 : # Done: 2016-07-08T03:41:42-0700 (4m 4s ago)\n",
      "2016-07-08 03:45:46.742811 : process/ext_person_mention_by_map_person_mention/run.sh\n",
      "2016-07-08 03:45:46.742820 : mark_done process/ext_person_mention_by_map_person_mention\n",
      "2016-07-08 03:45:46.742829 : ##############################################################################\n",
      "2016-07-08 03:45:46.742839 \n",
      "2016-07-08 03:45:46.742848 : ## data/person_mention #######################################################\n",
      "2016-07-08 03:45:46.742857 : # Done: 2016-07-08T03:41:42-0700 (4m 4s ago)\n",
      "2016-07-08 03:45:46.742868 : # no-op\n",
      "2016-07-08 03:45:46.742884 : mark_done data/person_mention\n",
      "2016-07-08 03:45:46.742897 : ##############################################################################\n",
      "2016-07-08 03:45:46.742909 \n",
      "2016-07-08 03:45:46.742919 ## process/ext_spouse_feature_by_extract_spouse_features #####################\n",
      "2016-07-08 03:45:46.742951 # Done: N/A\n",
      "2016-07-08 03:45:46.742976 process/ext_spouse_feature_by_extract_spouse_features/run.sh\n",
      "2016-07-08 03:45:46.742989 ++ dirname process/ext_spouse_feature_by_extract_spouse_features/run.sh\n",
      "2016-07-08 03:45:46.743001 + cd process/ext_spouse_feature_by_extract_spouse_features\n",
      "2016-07-08 03:45:46.743018 + : dd_tmp_ dd_old_\n",
      "2016-07-08 03:45:46.743040 + export DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_spouse_feature_by_extract_spouse_features\n",
      "2016-07-08 03:45:46.743060 + DEEPDIVE_CURRENT_PROCESS_NAME=process/ext_spouse_feature_by_extract_spouse_features\n",
      "2016-07-08 03:45:46.743080 + export DEEPDIVE_LOAD_FORMAT=tsj\n",
      "2016-07-08 03:45:46.743098 + DEEPDIVE_LOAD_FORMAT=tsj\n",
      "2016-07-08 03:45:46.743128 + output_relation=spouse_feature\n",
      "2016-07-08 03:45:46.743147 + output_relation_tmp=dd_tmp_spouse_feature\n",
      "2016-07-08 03:45:46.743156 + output_relation_old=dd_old_spouse_feature\n",
      "2016-07-08 03:45:46.743176 + deepdive create table-if-not-exists spouse_feature\n",
      "2016-07-08 03:45:47.005817 CREATE TABLE\n",
      "2016-07-08 03:45:47.006628 + deepdive create table dd_tmp_spouse_feature like spouse_feature\n",
      "2016-07-08 03:45:47.216071 CREATE TABLE\n",
      "2016-07-08 03:45:47.217013 + deepdive compute execute 'input_sql=\n",
      "2016-07-08 03:45:47.217050 \n",
      "2016-07-08 03:45:47.217074 SELECT R0.mention_id AS column_0\n",
      "2016-07-08 03:45:47.217095      , R1.mention_id AS column_1\n",
      "2016-07-08 03:45:47.217116      , R0.begin_index AS column_2\n",
      "2016-07-08 03:45:47.217132      , R0.end_index AS column_3\n",
      "2016-07-08 03:45:47.217145      , R1.begin_index AS column_4\n",
      "2016-07-08 03:45:47.217164      , R1.end_index AS column_5\n",
      "2016-07-08 03:45:47.217183      , R0.doc_id AS column_6\n",
      "2016-07-08 03:45:47.217201      , R0.sentence_index AS column_7\n",
      "2016-07-08 03:45:47.217219      , R2.tokens AS column_8\n",
      "2016-07-08 03:45:47.217240      , R2.lemmas AS column_9\n",
      "2016-07-08 03:45:47.217258      , R2.pos_tags AS column_10\n",
      "2016-07-08 03:45:47.217269      , R2.ner_tags AS column_11\n",
      "2016-07-08 03:45:47.217281      , R2.dep_types AS column_12\n",
      "2016-07-08 03:45:47.217292      , R2.dep_tokens AS column_13\n",
      "2016-07-08 03:45:47.217303 FROM person_mention R0\n",
      "2016-07-08 03:45:47.217315    , person_mention R1\n",
      "2016-07-08 03:45:47.217326    , sentences R2\n",
      "2016-07-08 03:45:47.217337 WHERE R1.doc_id = R0.doc_id\n",
      "2016-07-08 03:45:47.217348   AND R1.sentence_index = R0.sentence_index\n",
      "2016-07-08 03:45:47.217359   AND R2.doc_id = R0.doc_id\n",
      "2016-07-08 03:45:47.217369   AND R2.sentence_index = R0.sentence_index\n",
      "2016-07-08 03:45:47.217380 \n",
      "2016-07-08 03:45:47.217390 ' 'command=cd \"$DEEPDIVE_APP\" && udf/extract_spouse_features.py' output_relation=dd_tmp_spouse_feature\n",
      "2016-07-08 03:45:47.403017 Executing with the following configuration:\n",
      "2016-07-08 03:45:47.403104  DEEPDIVE_NUM_PROCESSES=7\n",
      "2016-07-08 03:45:47.403126  DEEPDIVE_NUM_PARALLEL_UNLOADS=1\n",
      "2016-07-08 03:45:47.403138  DEEPDIVE_NUM_PARALLEL_LOADS=1\n",
      "2016-07-08 03:45:47.590976 unloading to feed_processes-1: '\n",
      "2016-07-08 03:45:47.591033 \n",
      "2016-07-08 03:45:47.591058 SELECT R0.mention_id AS column_0\n",
      "2016-07-08 03:45:47.591078      , R1.mention_id AS column_1\n",
      "2016-07-08 03:45:47.591099      , R0.begin_index AS column_2\n",
      "2016-07-08 03:45:47.591119      , R0.end_index AS column_3\n",
      "2016-07-08 03:45:47.591135      , R1.begin_index AS column_4\n",
      "2016-07-08 03:45:47.591151      , R1.end_index AS column_5\n",
      "2016-07-08 03:45:47.591169      , R0.doc_id AS column_6\n",
      "2016-07-08 03:45:47.591186      , R0.sentence_index AS column_7\n",
      "2016-07-08 03:45:47.591202      , R2.tokens AS column_8\n",
      "2016-07-08 03:45:47.591221      , R2.lemmas AS column_9\n",
      "2016-07-08 03:45:47.591240      , R2.pos_tags AS column_10\n",
      "2016-07-08 03:45:47.591255      , R2.ner_tags AS column_11\n",
      "2016-07-08 03:45:47.591270      , R2.dep_types AS column_12\n",
      "2016-07-08 03:45:47.591323      , R2.dep_tokens AS column_13\n",
      "2016-07-08 03:45:47.591345 FROM person_mention R0\n",
      "2016-07-08 03:45:47.591364    , person_mention R1\n",
      "2016-07-08 03:45:47.591382    , sentences R2\n",
      "2016-07-08 03:45:47.591399 WHERE R1.doc_id = R0.doc_id\n",
      "2016-07-08 03:45:47.591414   AND R1.sentence_index = R0.sentence_index\n",
      "2016-07-08 03:45:47.591429   AND R2.doc_id = R0.doc_id\n",
      "2016-07-08 03:45:47.591444   AND R2.sentence_index = R0.sentence_index\n",
      "2016-07-08 03:45:47.591461 \n",
      "2016-07-08 03:45:47.591479 '\n",
      "2016-07-08 03:45:47.825284 Loading dd_tmp_spouse_feature from output_computed-1 (tsj format)\n",
      "2016-07-08 03:45:50.618501 COPY 210503\n",
      "2016-07-08 03:45:50.628426 + : 'Replacing spouse_feature with dd_tmp_spouse_feature'\n",
      "2016-07-08 03:45:50.628502 + deepdive sql 'DROP TABLE IF EXISTS dd_old_spouse_feature CASCADE;'\n",
      "2016-07-08 03:45:50.728100 DROP TABLE\n",
      "2016-07-08 03:45:50.728943 + deepdive sql 'ALTER TABLE spouse_feature     RENAME TO dd_old_spouse_feature;'\n",
      "2016-07-08 03:45:50.831459 ALTER TABLE\n",
      "2016-07-08 03:45:50.832547 + deepdive sql 'ALTER TABLE dd_tmp_spouse_feature RENAME TO spouse_feature;'\n",
      "2016-07-08 03:45:50.907693 ALTER TABLE\n",
      "2016-07-08 03:45:50.908305 + deepdive sql 'DROP TABLE IF EXISTS dd_old_spouse_feature CASCADE;'\n",
      "2016-07-08 03:45:50.982166 DROP TABLE\n",
      "2016-07-08 03:45:50.983125 + deepdive db analyze spouse_feature\n",
      "2016-07-08 03:45:52.444074 ANALYZE\n",
      "2016-07-08 03:45:52.445357 mark_done process/ext_spouse_feature_by_extract_spouse_features\n",
      "2016-07-08 03:45:52.460544 ##############################################################################\n",
      "2016-07-08 03:45:52.460613 \n",
      "2016-07-08 03:45:52.460635 ## data/spouse_feature #######################################################\n",
      "2016-07-08 03:45:52.460651 # Done: N/A\n",
      "2016-07-08 03:45:52.460667 # no-op\n",
      "2016-07-08 03:45:52.460683 mark_done data/spouse_feature\n",
      "2016-07-08 03:45:52.472887 ##############################################################################\n",
      "2016-07-08 03:45:52.472946 \n",
      "���run/FINISHED��� -> ���run/FINISHED~���\n",
      "���run/FINISHED��� -> ���20160708/034546.442183000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo spouse_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at a sample of the extracted features, they will look roughly like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature                   \r\n",
      "--------------------------------------------\r\n",
      " IS_INVERTED\r\n",
      " INV_WORD_SEQ_[Carson]\r\n",
      " INV_LEMMA_SEQ_[Carson]\r\n",
      " INV_NER_SEQ_[PERSON]\r\n",
      " INV_POS_SEQ_[NNP]\r\n",
      " INV_W_LEMMA_L_1_R_1_[Dr.]_[,]\r\n",
      " INV_W_NER_L_1_R_1_[O]_[O]\r\n",
      " INV_W_LEMMA_L_1_R_2_[Dr.]_[, I]\r\n",
      " INV_W_NER_L_1_R_2_[O]_[O O]\r\n",
      " INV_W_LEMMA_L_1_R_3_[Dr.]_[, I write]\r\n",
      " INV_W_NER_L_1_R_3_[O]_[O O O]\r\n",
      " INV_W_LEMMA_L_2_R_1_[Dear Dr.]_[,]\r\n",
      " INV_W_NER_L_2_R_1_[O O]_[O]\r\n",
      " INV_W_LEMMA_L_2_R_2_[Dear Dr.]_[, I]\r\n",
      " INV_W_NER_L_2_R_2_[O O]_[O O]\r\n",
      " INV_W_LEMMA_L_2_R_3_[Dear Dr.]_[, I write]\r\n",
      " INV_W_NER_L_2_R_3_[O O]_[O O O]\r\n",
      " INV_W_LEMMA_L_3_R_1_[. Dear Dr.]_[,]\r\n",
      " INV_W_NER_L_3_R_1_[O O O]_[O]\r\n",
      " INV_W_LEMMA_L_3_R_2_[. Dear Dr.]_[, I]\r\n",
      "(20 rows)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!deepdive query '| 20 ?- spouse_feature(_, _, feature).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have generated what looks more like the standard input to a machine learning problem—a set of objects, represented by sets of features, which we want to classify (here, as true or false mentions of a spousal relation).\n",
    "However, we **don't have any supervised labels** (i.e., a set of correct answers) for a machine learning algorithm to learn from!\n",
    "In most real world applications, a sufficiently large set of supervised labels is _not_ available.\n",
    "With DeepDive, we take the approach sometimes referred to as _distant supervision_ or _data programming_, where we instead generate a **noisy set of labels using a mix of mappings from secondary datasets and other heuristic rules**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distant supervision with data and rules\n",
    "In this section, we'll use _distant supervision_ (or '_data programming_') to provide a noisy set of labels for candidate relation mentions, with which we will train a machine learning model.\n",
    "\n",
    "We'll describe two basic categories of approaches:\n",
    "\n",
    "1. Mapping from secondary data for distant supervision\n",
    "2. Using heuristic rules for distant supervision\n",
    "\n",
    "Finally, we'll describe a simple majority-vote approach to resolving multiple labels per example, which can be implemented within DDlog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Mapping from secondary data for distant supervision\n",
    "First, we'll try using an external structured dataset of known married couples, from [DBpedia](http://wiki.dbpedia.org/), to distantly supervise our dataset.\n",
    "We'll download the relevant data, and then map it to our candidate spouse relations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting and downloading the DBpedia data\n",
    "Our goal is to first extract a collection of known married couples from DBpedia and then load this into the `spouses_dbpedia` table in our database.\n",
    "To extract known married couples, we use the DBpedia dump present in [Google's BigQuery platform](https://bigquery.cloud.google.com).\n",
    "First we extract the URI, name and spouse information from the DBpedia `person` table records in BigQuery for which the field `name` is not NULL.\n",
    "We use the following query:\n",
    "\n",
    "```sql\n",
    "SELECT URI,name, spouse\n",
    "FROM [fh-bigquery:dbpedia.person]\n",
    "where name <> \"NULL\"\n",
    "```\n",
    "\n",
    "We store the result of the above query in a local project table `dbpedia.validnames` and perform a self-join to obtain the pairs of married couples.\n",
    "\n",
    "```sql\n",
    "SELECT t1.name, t2.name\n",
    "FROM [dbpedia.validnames] AS t1\n",
    "JOIN EACH [dbpedia.validnames] AS t2\n",
    "ON t1.spouse = t2.URI\n",
    "```\n",
    "\n",
    "The output of the above query is stored in a new table named `dbpedia.spouseraw`.\n",
    "Finally, we use the following query to remove symmetric duplicates.\n",
    "\n",
    "```sql\n",
    "SELECT p1, p2\n",
    "FROM (SELECT t1_name as p1, t2_name as p2 FROM [dbpedia.spouseraw]),\n",
    "     (SELECT t2_name as p1, t1_name as p2 FROM [dbpedia.spouseraw])\n",
    "WHERE p1 < p2\n",
    "```\n",
    "\n",
    "The output of this query is stored in a local file.\n",
    "The file contains duplicate rows (BigQuery does not support `distinct`).\n",
    "It also contains noisy rows where the name field contains a string where the given name family name and multiple aliases were concatenated and reported in a string including the characters `{` and `}`.\n",
    "Using the Unix commands `sed`, `sort` and `uniq` we first remove the lines containing characters `{` and `}` and then duplicate entries.\n",
    "This results in an input file `spouses_dbpedia.csv` containing 6,126 entries of married couples.\n",
    "\n",
    "*Note that we made this [`spouses_dbpedia.csv` available for download from GitHub](https://github.com/HazyResearch/deepdive/blob/master/examples/spouse/input/spouses_dbpedia.csv.bz2), so you don't have to repeat the above process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading DBpedia data to database\n",
    "\n",
    "To load the known married couples data into DeepDive, we first declare the schema in DDlog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.ddlog\n"
     ]
    }
   ],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "## Distant Supervision ########################################################\n",
    "\n",
    "spouses_dbpedia(\n",
    "    person1_name text,\n",
    "    person2_name text\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can easily load the data in `spouses_dbpedia.csv` data to the table we just declared if we follow DeepDive's convention of organizing input data under `input/` directory.\n",
    "The input file name simply needs to start with the target database table name.\n",
    "Let's download the file from GitHub to `input/spouses_dbpedia.csv.bz2` under our application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   174  100   174    0     0    358      0 --:--:-- --:--:-- --:--:--   358\n",
      "100 77463  100 77463    0     0  64647      0  0:00:01  0:00:01 --:--:-- 6994k\n"
     ]
    }
   ],
   "source": [
    "!cd input && curl -RLO \"https://github.com/HazyResearch/deepdive/raw/master/examples/spouse/input/spouses_dbpedia.csv.bz2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then execute this command to load it into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mapp.ddlog: updated since last `deepdive compile`\n",
      "\u001b[0m2016-07-08 04:13:33.214350 ���run/LATEST.COMPILE��� -> ���20160708/041333.169215000���\n",
      "2016-07-08 04:13:33.214554 ���run/RUNNING.COMPILE��� -> ���20160708/041333.169215000���\n",
      "2016-07-08 04:13:33.214594 Parsing DeepDive application (/ConfinedWater/spouse) to generate:\n",
      "2016-07-08 04:13:33.214626  run/compiled/deepdive.conf\n",
      "2016-07-08 04:13:33.214649   from app.ddlog\n",
      "2016-07-08 04:13:33.686394  run/compiled/deepdive.conf.json\n",
      "2016-07-08 04:13:33.880471 Performing sanity checks on run/compiled/deepdive.conf.json:\n",
      "2016-07-08 04:13:33.913667  checking if input_extractors_well_defined\n",
      "2016-07-08 04:13:33.913723  checking if input_schema_wellformed\n",
      "2016-07-08 04:13:33.914035 Normalizing and adding built-in processes to the data flow to compile:\n",
      "2016-07-08 04:13:33.914463  run/compiled/config-0.00-init_objects.json\n",
      "2016-07-08 04:13:33.928578  run/compiled/config-0.01-parse_calibration.json\n",
      "2016-07-08 04:13:33.942831  run/compiled/config-0.01-parse_schema.json\n",
      "2016-07-08 04:13:33.961554  run/compiled/config-0.51-add_init_app.json\n",
      "2016-07-08 04:13:33.975299  run/compiled/config-0.52-input_loader.json\n",
      "2016-07-08 04:13:33.989804  run/compiled/config-1.00-qualified_names.json\n",
      "2016-07-08 04:13:34.006696  run/compiled/config-1.01-parse_inference_rules.json\n",
      "2016-07-08 04:13:34.032128  run/compiled/config-2.01-grounding.json\n",
      "2016-07-08 04:13:34.116348  run/compiled/config-2.02-learning_inference.json\n",
      "2016-07-08 04:13:34.154628  run/compiled/config-2.03-calibration_plots.json\n",
      "2016-07-08 04:13:34.169091  run/compiled/config-9.98-ensure_init_app.json\n",
      "2016-07-08 04:13:34.183572  run/compiled/config-9.99-dependencies.json\n",
      "2016-07-08 04:13:34.199076  run/compiled/config.json\n",
      "2016-07-08 04:13:34.201740 Validating run/compiled/config.json:\n",
      "2016-07-08 04:13:34.242975  checking if compiled_base_relations_have_input_data\n",
      "2016-07-08 04:13:34.243039  checking if compiled_dependencies_correct\n",
      "2016-07-08 04:13:34.243061  checking if compiled_input_output_well_defined\n",
      "2016-07-08 04:13:34.243076  checking if compiled_output_uniquely_defined\n",
      "2016-07-08 04:13:34.243086  [ERROR] base relation 'spouses_dbpedia' must have data to load at: input/spouses_dbpedia.*\n",
      "2016-07-08 04:13:34.243097  [ERROR] FAILED deepdive check compiled_base_relations_have_input_data\n",
      "���run/ABORTED.COMPILE��� -> ���20160708/041333.169215000���\n"
     ]
    }
   ],
   "source": [
    "!deepdive redo spouses_dbpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the database should include tuples that look like the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!deepdive query '| 20 ?- spouses_dbpedia(name1, name2).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervising spouse candidates with DBpedia data\n",
    "First we'll declare a new table where we'll store the labels (referring to the spouse candidate mentions), with an integer value (`True=1, False=-1`) and a description (`rule_id`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "spouse_label(\n",
    "    p1_id   text,\n",
    "    p2_id   text,\n",
    "    label   int,\n",
    "    rule_id text\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the spouse candidate mentions with a `NULL` label.  This is just for simplifying some steps later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# make sure all pairs in spouse_candidate are considered as unsupervised examples\n",
    "spouse_label(p1,p2, 0, NULL) :-\n",
    "    spouse_candidate(p1, _, p2, _).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll implement a simple distant supervision rule which labels any spouse mention candidate with a pair of names appearing in DBpedia as true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# distant supervision using data from DBpedia\n",
    "spouse_label(p1,p2, 1, \"from_dbpedia\") :-\n",
    "    spouse_candidate(p1, p1_name, p2, p2_name),\n",
    "    spouses_dbpedia(n1, n2),\n",
    "    [ lower(n1) = lower(p1_name), lower(n2) = lower(p2_name) ;\n",
    "      lower(n2) = lower(p1_name), lower(n1) = lower(p2_name) ].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that there are many clear ways in which this rule could be improved (fuzzy matching, more restrictive conditions, etc.), but this serves as an example of one major type of distant supervision rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Using heuristic rules for distant supervision\n",
    "We can also create a supervision rule which does not rely on any secondary structured dataset like DBpedia, but instead just uses some heuristic.\n",
    "We set up a DDlog function, `supervise`, which uses a UDF containing several heuristic rules over the mention and sentence attributes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# supervision by heuristic rules in a UDF\n",
    "function supervise over (\n",
    "        p1_id text, p1_begin int, p1_end int,\n",
    "        p2_id text, p2_begin int, p2_end int,\n",
    "        doc_id         text,\n",
    "        sentence_index int,\n",
    "        sentence_text  text,\n",
    "        tokens         text[],\n",
    "        lemmas         text[],\n",
    "        pos_tags       text[],\n",
    "        ner_tags       text[],\n",
    "        dep_types      text[],\n",
    "        dep_tokens     int[]\n",
    "    ) returns (\n",
    "        p1_id text, p2_id text, label int, rule_id text\n",
    "    )\n",
    "    implementation \"udf/supervise_spouse.py\" handles tsj lines.\n",
    "\n",
    "spouse_label += supervise(\n",
    "    p1_id, p1_begin, p1_end,\n",
    "    p2_id, p2_begin, p2_end,\n",
    "    doc_id, sentence_index,\n",
    "    tokens, lemmas, pos_tags, ner_tags, dep_types, dep_token_indexes\n",
    ") :-\n",
    "    spouse_candidate(p1_id, _, p2_id, _),\n",
    "    person_mention(p1_id, p1_text, doc_id, sentence_index, p1_begin, p1_end),\n",
    "    person_mention(p2_id, p2_text,      _,              _, p2_begin, p2_end),\n",
    "    sentences(\n",
    "        doc_id, sentence_index,\n",
    "        tokens, lemmas, pos_tags, ner_tags, _, dep_types, dep_token_indexes\n",
    "    ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python UDF named [`udf/supervise_spouse.py`](udf/supervise_spouse.py) contains several heuristic rules:\n",
    "\n",
    "* Candidates with person mentions that are too far apart in the sentence are marked as false.\n",
    "* Candidates with person mentions that have another person in between are marked as false.\n",
    "* Candidates with person mentions that have words like \"wife\" or \"husband\" in between are marked as true.\n",
    "* Candidates with person mentions that have \"and\" in between and \"married\" after are marked as true.\n",
    "* Candidates with person mentions that have familial relation words in between are marked as false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file udf/supervise_spouse.py\n",
    "#!/usr/bin/env python\n",
    "from deepdive import *\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "SpouseLabel = namedtuple('SpouseLabel', 'p1_id, p2_id, label, type')\n",
    "\n",
    "@tsj_extractor\n",
    "@returns(lambda\n",
    "        p1_id   = \"text\",\n",
    "        p2_id   = \"text\",\n",
    "        label   = \"int\",\n",
    "        rule_id = \"text\",\n",
    "    :[])\n",
    "# heuristic rules for finding positive/negative examples of spouse relationship mentions\n",
    "def supervise(\n",
    "        p1_id=\"text\", p1_begin=\"int\", p1_end=\"int\",\n",
    "        p2_id=\"text\", p2_begin=\"int\", p2_end=\"int\",\n",
    "        doc_id=\"text\", sentence_index=\"int\",\n",
    "        tokens=\"text[]\", lemmas=\"text[]\", pos_tags=\"text[]\", ner_tags=\"text[]\",\n",
    "        dep_types=\"text[]\", dep_token_indexes=\"int[]\",\n",
    "    ):\n",
    "\n",
    "    # Constants\n",
    "    MARRIED = frozenset([\"wife\", \"husband\"])\n",
    "    FAMILY = frozenset([\"mother\", \"father\", \"sister\", \"brother\", \"brother-in-law\"])\n",
    "    MAX_DIST = 10\n",
    "\n",
    "    # Common data objects\n",
    "    p1_end_idx = min(p1_end, p2_end)\n",
    "    p2_start_idx = max(p1_begin, p2_begin)\n",
    "    p2_end_idx = max(p1_end,p2_end)\n",
    "    intermediate_lemmas = lemmas[p1_end_idx+1:p2_start_idx]\n",
    "    intermediate_ner_tags = ner_tags[p1_end_idx+1:p2_start_idx]\n",
    "    tail_lemmas = lemmas[p2_end_idx+1:]\n",
    "    spouse = SpouseLabel(p1_id=p1_id, p2_id=p2_id, label=None, type=None)\n",
    "\n",
    "    # Rule: Candidates that are too far apart\n",
    "    if len(intermediate_lemmas) > MAX_DIST:\n",
    "        yield spouse._replace(label=-1, type='neg:far_apart')\n",
    "\n",
    "    # Rule: Candidates that have a third person in between\n",
    "    if 'PERSON' in intermediate_ner_tags:\n",
    "        yield spouse._replace(label=-1, type='neg:third_person_between')\n",
    "\n",
    "    # Rule: Sentences that contain wife/husband in between\n",
    "    #         (<P1>)([ A-Za-z]+)(wife|husband)([ A-Za-z]+)(<P2>)\n",
    "    if len(MARRIED.intersection(intermediate_lemmas)) > 0:\n",
    "        yield spouse._replace(label=1, type='pos:wife_husband_between')\n",
    "\n",
    "    # Rule: Sentences that contain and ... married\n",
    "    #         (<P1>)(and)?(<P2>)([ A-Za-z]+)(married)\n",
    "    if (\"and\" in intermediate_lemmas) and (\"married\" in tail_lemmas):\n",
    "        yield spouse._replace(label=1, type='pos:married_after')\n",
    "\n",
    "    # Rule: Sentences that contain familial relations:\n",
    "    #         (<P1>)([ A-Za-z]+)(brother|stster|father|mother)([ A-Za-z]+)(<P2>)\n",
    "    if len(FAMILY.intersection(intermediate_lemmas)) > 0:\n",
    "        yield spouse._replace(label=-1, type='neg:familial_between')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rough theory behind this approach is that we don't need high-quality (e.g., hand-labeled) supervision to learn a high quality model.\n",
    "Instead, using statistical learning, we can in fact recover high-quality models from a large set of low-quality or **_noisy_** labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Resolving multiple labels per example with majority vote\n",
    "Finally, we implement a very simple majority vote procedure, all in DDlog, for resolving scenarios where a single spouse candidate mention has multiple conflicting labels.\n",
    "First, we sum the labels (which are all -1, 0, or 1):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# resolve multiple labels by majority vote (summing the labels in {-1,0,1})\n",
    "spouse_label_resolved(p1_id, p2_id, SUM(vote)) :-\n",
    "    spouse_label(p1_id, p2_id, vote, rule_id).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we simply threshold and add these labels to our decision variable table `has_spouse` (see next section for details here):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# assign the resolved labels for the spouse relation\n",
    "has_spouse(p1_id, p2_id) = if l > 0 then TRUE\n",
    "                      else if l < 0 then FALSE\n",
    "                      else NULL end :- spouse_label_resolved(p1_id, p2_id, l)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, to execute all of the above, just run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!deepdive redo has_spouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `deepdive do` will execute all upstream tasks as well, so this will execute all of the previous steps!\n",
    "\n",
    "Now, we can take a brief look at how many candidates are supervised by different rules, which will look something like the table below.\n",
    "Obviously, the counts will vary depending on your input corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!deepdive query 'rule, @order_by COUNT(1) ?- spouse_label(p1,p2, label, rule).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning and inference: model specification\n",
    "Now, we need to specify the actual model that DeepDive will perform learning and inference over.\n",
    "At a high level, this boils down to specifying three things:\n",
    "\n",
    "1. What are the _variables_ of interest that we want DeepDive to predict for us?\n",
    "\n",
    "2. What are the _features_ for each of these variables?\n",
    "\n",
    "3. What are the _connections_ between the variables?\n",
    "\n",
    "One we have specified the model in this way, DeepDive will _learn_ the parameters of the model (the weights of the features and potentially the connections between variables), and then perform _statistical inference_ over the learned model to determine the probability that each variable of interest is true.\n",
    "\n",
    "For more advanced users: we are specifying a _factor graph_ where the features are unary factors, and then using SGD and Gibbs sampling for learning and inference.\n",
    "Further technical detail is available [here](#).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Specifying prediction variables\n",
    "In our case, we have one variable to predict per spouse candidate mention, namely, **is this mention actually indicating a spousal relation or not?**\n",
    "In other words, we want DeepDive to predict the value of a Boolean variable for each spouse candidate mention, indicating whether it is true or not.\n",
    "Recall that we started this tutorial with specifying this at the beginning of [`app.ddlog`](app.ddlog) as follows:\n",
    "\n",
    "```ddlog\n",
    "has_spouse?(\n",
    "    p1_id text,\n",
    "    p2_id text\n",
    ").\n",
    "```\n",
    "\n",
    "DeepDive will predict not only the value of these variables, but also the marginal probabilities, i.e., the confidence level that DeepDive has for each individual prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Specifying features\n",
    "Next, we indicate (i) that each `has_spouse` variable will be connected to the features of the corresponding `spouse_candidate` row, (ii) that we wish DeepDive to learn the weights of these features from our distantly supervised data, and (iii) that the weight of a specific feature across all instances should be the same, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "## Inference Rules ############################################################\n",
    " \n",
    "# Features\n",
    "@weight(f)\n",
    "has_spouse(p1_id, p2_id) :-\n",
    "    spouse_feature(p1_id, p2_id, f).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Specifying connections between variables\n",
    "Finally, we can specify dependencies between the prediction variables, with either learned or given weights.\n",
    "Here, we'll specify two such rules, with fixed (given) weights that we specify.\n",
    "First, we define a _symmetry_ connection, namely specifying that if the model thinks a person mention `p1` and a person mention `p2` indicate a spousal relationship in a sentence, then it should also think that the reverse is true, i.e., that `p2` and `p1` indicate one too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# Inference rule: Symmetry\n",
    "@weight(3.0)\n",
    "has_spouse(p1_id, p2_id) => has_spouse(p2_id, p1_id) :-\n",
    "    TRUE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify a rule that the model should be strongly biased towards finding one marriage indication per person mention.\n",
    "We do this inversely, using a negative weight, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%file -a app.ddlog\n",
    "\n",
    "# Inference rule: Only one marriage\n",
    "@weight(-1.0)\n",
    "has_spouse(p1_id, p2_id) => has_spouse(p1_id, p3_id) :-\n",
    "    TRUE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Performing learning and inference\n",
    "\n",
    "Finally, to perform learning and inference using the specified model, we need to run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!deepdive redo probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ground the model based on the data in the database, learn the weights, infer the expectations or marginal probabilities of the variables in the model, and then load them back to the database.\n",
    "\n",
    "Let's take a look at the probabilities inferred by DeepDive for the `has_spouse` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!deepdive sql 'SELECT p1_id, p2_id, expectation FROM has_spouse_inference ORDER BY random() LIMIT 20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
