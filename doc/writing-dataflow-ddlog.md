---
layout: default
title: Defining Data Processing in DDlog
---

# DDlog

DDlog is a higher-level language for writing DeepDive applications in succinct Datalog-like syntax.
We are gradually extending the language to allow expression of advanced SQL queries used by complex extractors as well as a variety of factors with Markov Logic-like syntax.
A reference for ddlog lanugage features can be found [here](https://github.com/HazyResearch/ddlog/wiki/DDlog-Language-Features).

## Writing a DDlog Program

A DDlog program consists of the following parts:

1. Schema declarations (eg. relational tables)
2. Normal Datalog Rules
  1. Conditions
3. Data transformation rules
  1. User Defined Functions (UDFs)
  2. Function Call Rules


All DDlog code should be placed in a file named `app.ddlog` under the DeepDive application.
A complete example written in DDlog can be found from [examples/spouse_example/postgres/ddlog](https://github.com/HazyResearch/deepdive/blob/master/examples/spouse_example/postgres/ddlog)


### Basic Syntax

Each declaration and rule ends with a period (`.`).
The order of the rules have no meaning.
Comments in DDlog begin with a hash (`#`) character.

### Schema Declaration

First of all, we declare the relations we use throughout the program.  These relations refer to tables that will appear in your database.  See [deepdive sql TODO](fill in with real link) for details on how to interact with the database.
The order doesn't matter, but it's a good idea to place them at the beginning because that makes it easier to understand.

#### Relations Syntax
```
relation_name(
  column1_name  column1_type,
  column2_name  column2_type,
  ...).
```
Similar to a SQL table definition, a schema declaration is just the name of a relation followed by a comma separated list of the columns and their types.

Below is a realistic example.

```
article(
  id int,
  length int,
  author text,
  words text[]).
```
Here we are defining a relation named "article" with four columns, named "id", "length", "author" and "words" respectively. Each column has it's own type, here utilizing `int`, `text` and `text[]`.
[//]: # (TODO  Maybe link this to a real example in spouse example instead)

<!--
#### Variable Relations
We can declare a variable relation that we want DeepDive to predict the marginal probability for us.
The syntax is as follows.
```
relation_name?(column_name column_type)
```

An example would be.
```
has_spouse?(relation_id text)
```

Here we are defining a relation has_spouse which will be a table created in the database having the column "relation_id" but it will also have several other columns created by DeepDive for internal use.  Ultimately, the inferences that are generated by DeepDive will be stored in this relation.
-->

## Normal datalog Rules

Typical datalog rules can be used for defining how a relation is derived from
other relations.
The head is the relation being defined, and the body is a conjunction of conjunctive query bodies,
separated by commas.
Disjunctive cases are expressed with multiple bodies separated by semicolons.

For example, the following program states that the tuples of `Q` are derived
from `R` and `S`, where the second column of `R` is *unified* with the first column
of `S`, i.e., the body is a equi-join between `R` and `S`.
```
Q(x, y) :- R(x, y), S(y).
```
Here, `Q(x, y)` is the head, and `R(x, y)` and `S(y)` are body *atoms*. `x` and `y`
are *variables* of the rule.

### Expressions
Expressions can be used in the atom columns. Expressions in the head atom will be output
to the head relation. Expressions in the body atom will become a condition.
Note datalog's semantics is based on unification of variables/expressions:
variables with the same name are unified, and an expression in the body atom
is unified with its corresponding column. A comprehensive example is shown below.
```
a(k int).
b(k int, p text, q text, r int).
c(s text, n int, t text).

Q("test", f(123), id) :- a(id), b(id, x,y,z), c(x || y,10,"foo").
```
Here the ouptut is a tuple of string literal "text", a function `f` operating on 123,
and id produced by body.
In the body, the conditions are compiled to
```
a.k = b.k -- unifying variable id in a(id) and b(id, ...)
AND c.n = 10  -- unifying literal 10 with the column in c(...,10,...)
AND c.t = 'foo' -- unifying literal "foo" with the column in c(...,"foo")
AND b.p || b.q = c.s  -- unifying expression x||y with the column in c(x||y,...)
```
Expressions can be arbitrarily nested.

### Conditions
Conditions are put at the same level of body atoms, and collectively with body atoms
defined the output to the head. Conjunctive conditions are seperated by comma, and
disjunctive conditions are separated by semicolon. Conditions can be arbitrarily
nested by putting inside square brackets. For example,
```
Q(x) :- b(x,y,z,w), [x + w = 100; [!x > 50, w < 10]].
```
The conditions are `(x + w = 100) OR ((NOT x > 50) AND w < 10)`.

### Placeholder
A placeholder `_` can be used in columns of body atoms, indicating the output is
irrelevant to that column.

## Aggregation
Using aggregation functions in the head atom columns indicates doing a group by
on the rest of columns, and an aggregate .
Supported aggregation functions are SUM, MAX, MIN, ARRAY_AGG, COUNT.
For example,
```
Q(a,b,MAX(c)) :- R(a,b,c).
```
will group by the first and second columns of `R`, and take the max of the third column.


### User Defined Functions (UDFs)
DeepDive allows the user to define their own functions.  These functions will read a relation line by line and then output to another relation.  Therefore, when we define a function in DDlog, we must define it's input structure, output structure and how it is implemented.

If we are writing a UDF to classify the articles above into a set of classes, we might have another relation
```
classified_articles(
  article_id int,
  class text).
```

The implementation of our UDF will be in a file called "udf/classify.py".  (Note: this is saying that it is a python script that is in the 'udf' folder but DeepDive can execute any type of executable).  The syntax for defining the function is
```
function classify_articles over (id int, author text, length int, words text[])
  return (article_id int, class text)
  implementation "udf/classify.py" handles tsv lines.
```

Notice that we've replicated the fields in the classified_articles.  Since we've already defined a relation that this function is intended to fill, we can simplify this declaration by using `rows like`

```
function classify_articles over (id int, author text, length int, words text[])
  return rows like classified_articles
  implementation "udf/classify.py" handles tsv lines.
```

Also note that the input is similar to the articles relation, but the fields are in a different order.  This is on purpose to show that the input need not match the relation defintion exactly.  We will see how this comes into play in the Function Call Rules below.

### Function Call Rules
The functions defined above can be used to fill rows into a relation.  The syntax below will be used to call the classify function to fill the classified_articles relation using data from the articles relation.
```
classified_articles += classify_articles(id, author, length, words) :-
  article(id, length, author words)
```

The classify_articles function is being called on the input from articles and the output is being fed into classified_articles.  Note that the articles relation fields are in a different order than the input for the classify_articles function.  This is to demonstrate that the columns are referenced by name so they can be reordered or omitted when calling the function as needed.

