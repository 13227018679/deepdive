#!/usr/bin/env jq
# compile-config-2.02-learning_inference -- Adds processes for performing inference with the grounded factor graph
##

include "constants";
include "sql";

# skip adding learning/inference processes unless one for grounding is there
if .deepdive_.execution.processes | has("process/grounding/combine_factorgraph") | not then . else

.deepdive_ as $deepdive
| .deepdive_.execution.processes += {

    # learning weights and doing inference (since we had to load the graph anyway)
    "process/model/learning": {
        dependencies_: ["model/factorgraph"],
        output_: "model/weights",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            mkdir -p weights
            [ -d factorgraph ] || error \"No factorgraph found\"
            # run inference engine for learning and inference
            flatten() { find -L \"$@\" -type f -exec pbzip2 -c -d -k {} +; }
            \($deepdive.sampler.sampler_cmd // "sampler-dw") \\
                gibbs \\
                -w <(flatten factorgraph/weights) \\
                -v <(flatten factorgraph/variables) \\
                -f <(flatten factorgraph/factors) \\
                -m factorgraph/meta \\
                -o weights \\
                \($deepdive.sampler.sampler_args // "#")
            mkdir -p probabilities
            mv -f weights/inference_result.out.text probabilities/
        "
    },

    # performing inference
    "process/model/inference": {
        dependencies_: ["model/factorgraph", "model/weights"],
        output_: "model/probabilities",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            [ -d factorgraph ] || error \"No factorgraph found\"
            if [[ factorgraph/weights -nt probabilities/inference_result.out.text ]]; then
                # no need to run inference unless the weights are fresher
                # XXX this skipping may cause confusion
                # run sampler for performing inference with given weights without learning
                flatten() { find -L \"$@\" -type f -exec pbzip2 -c -d -k {} +; }
                \($deepdive.sampler.sampler_cmd // "sampler-dw") \\
                    gibbs \\
                    -w <(flatten factorgraph/weights) \\
                    -v <(flatten factorgraph/variables) \\
                    -f <(flatten factorgraph/factors) \\
                    -m factorgraph/meta \\
                    -o weights \\
                    \($deepdive.sampler.sampler_args // "") \\
                    -l 0 \\
                    #
                mkdir -p probabilities
                mv -f weights/inference_result.out.text probabilities/
            fi
        "
    },

    # loading learning/inference results back to database
    "process/model/load_weights": {
        dependencies_: ["model/weights"],
        output_: "data/model/weights",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            # load weights to database
            deepdive create table dd_inference_result_weights \\
                id:BIGINT:'PRIMARY KEY' \\
                weight:'DOUBLE PRECISION' \\
                #
            cat weights/inference_result.out.weights.text |
            tr \(" "|@sh) \("\\t"|@sh) | DEEPDIVE_LOAD_FORMAT=tsv \\
            deepdive load dd_inference_result_weights /dev/stdin

            # create views
            deepdive create view dd_inference_result_weights_mapping as \("
                SELECT dd_graph_weights.*, dd_inference_result_weights.weight FROM
                dd_graph_weights JOIN dd_inference_result_weights ON dd_graph_weights.id = dd_inference_result_weights.id
                ORDER BY abs(weight) DESC
            " | @sh)

            deepdive create view dd_inference_result_variables_mapped_weights as \("
                SELECT * FROM dd_inference_result_weights_mapping
                ORDER BY abs(weight) DESC
            " | @sh)
        "
    },
    "process/model/load_probabilities": {
        dependencies_: ["model/probabilities"],
        output_: "data/model/probabilities",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            # load weights to database
            deepdive create table dd_inference_result_variables \\
                id:BIGINT \\
                category:BIGINT \\
                expectation:'DOUBLE PRECISION' \\
                #
            cat probabilities/inference_result.out.text |
            tr \(" "|@sh) \("\\t"|@sh) | DEEPDIVE_LOAD_FORMAT=tsv \\
            deepdive load dd_inference_result_variables /dev/stdin

            # create a view for each app schema variable
            \([ $deepdive.schema.variables_[] | "
                deepdive create view \("\(.variablesTable)_inference" | @sh) as \(
                { SELECT:
                    [ { expr: "v.*" }
                    , { alias: "expectation",               table: "r", column: "expectation" }
                    , { alias: deepdiveVariableLabelColumn, table: "i", column: deepdiveVariableLabelColumn }
                    , { alias: deepdiveVariableIdColumn,    table: "i", column: deepdiveVariableIdColumn }
                    ]
                , FROM: [ { alias: "v", table: .variablesTable } ]
                , JOIN:
                    # variable ids
                    [ { LEFT_OUTER: { alias: "i", table: .variablesIdsTable }
                      , ON: { and:  [ .variablesKeyColumns[]
                                    | { eq: [ { table: "i", column: . }
                                            , { table: "v", column: . }
                                            ] }
                                    ] } }
                    , if .variableType == "boolean" then
                    # inference results
                      { LEFT_OUTER: { alias: "r", table: "dd_inference_result_variables" }
                      , ON: { eq:   [ { table: "r", column: "id" }
                                    , { table: "i", column: deepdiveVariableIdColumn }
                                    ] } }
                    else
                    # category ids are necessary to find the inference result corresponding to the variable
                      { LEFT_OUTER: { alias: "c", table: .variablesCategoriesTable }
                      , ON: { and:  [ .variablesCategoryColumns[]
                                    | { eq: [ { table: "c", column: "_\(.)" }
                                            , { table: "v", column: . }
                                            ] }
                                    ] } }
                    # inference results
                    , { LEFT_OUTER: { alias: "r", table: "dd_inference_result_variables" }
                      , ON: { and:  [ { eq: [ { table: "r", column: "id" }
                                            , { table: "i", column: deepdiveVariableIdColumn }
                                            ] }
                                    , { eq: [ { table: "r", column: "category" }
                                            , { table: "c", column: "cid" }
                                            ] }
                                    ] } }
                    end
                    ]
                , ORDER_BY:
                    { expr: { table: "r", column: "expectation" }
                    , order: "DESC"
                    }
                } | asSql | @sh)"
            ] | join("\n"))
        "
    }

}

end
