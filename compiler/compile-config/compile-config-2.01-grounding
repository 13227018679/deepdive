#!/usr/bin/env bash
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##
exec jq '
def merge(objects): reduce objects as $es ({}; . + $es);

# See: http://www.postgresql.org/docs/current/static/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS
def asSqlIdent: "\"\( tostring  # anything can be turned into a SQL identifier if they are inside double quotes
                    | gsub("\""; "\"\"") # even double quote themselves (in fact, PostgreSQL says \0 cannot be included)
                    )\"";

# a handy way to turn things into SQL string literals (as it is hard to write single quote within single quote in this Bash script)
def asSqlLiteral: "'\''\(tostring | gsub("'\''"; "'\'\''"))'\''";

.deepdive_ as $deepdive

# parse weight field for every inference rules as weight_ field
| .deepdive_.inference.factors |= with_entries(
    (.key | ltrimstr("factor/")) as $factorName | .value |=
    ( .weight_ = (.weight | gsub("^\\s+|\\s+$"; ""))
    | .weight_ |= (
        if startswith("?")? then
            # unknown weight, find parameters
            { is_fixed: false
            , params: (ltrimstr("?") | ltrimstr("(") | rtrimstr(")") | split("\\s*,\\s"))
            , init_value: 0.0
            }
        else
            # fixed weight
            { is_fixed: true
            , params: []
            , init_value: tonumber
            }
        end
        )
    )
)

| .deepdive_ as $deepdive  # necessary since we just mutated it


| .deepdive_.extraction.extractors += {

    # grounding the factor graph
    "process/grounding/legacy": {
        dependencies_: ($deepdive.inference.factors | keys),
        output_: "model/factorgraph",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            mkdir -p factorgraph

            # TODO run factor queries and grounding queries directly from here or via database drivers
            set +x; . load-db-driver.sh; set -x
            export DEEPDIVE_LOGFILE=factorgraph/grounding.log
            [[ ! -e \"$DEEPDIVE_LOGFILE\" ]] || mv -f \"$DEEPDIVE_LOGFILE\" \"$DEEPDIVE_LOGFILE\"~
            java org.deepdive.Main -c <(
                set +x
                echo \("deepdive \(.deepdive | @json)" | @sh)
                echo \("deepdive.pipeline.pipelines.grounding: [\(.deepdive.inference.factors | keys | join(", "))]" | @sh)
                echo \("deepdive.pipeline.run: grounding" | @sh)
            ) -o factorgraph -t inference_grounding

            # drop graph. prefix from file names
            cd factorgraph
            mv -f graph.variables variables
            mv -f graph.factors   factors
            mv -f graph.weights   weights
            mv -f graph.meta      meta
        "
    },

    # consecutive variable id range should be partitioned first by counting the variables
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all variable tables
            $deepdive.schema.variables | keys[] | "data/\(.)"
        ],
        style: "cmd_extractor",
        cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables | keys | map(@sh) | join(" ")) | {
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    },

    # each variable gets the consecutive ids assigned to its rows and dumped into tsv (below)

    # each inference rule input_query is run to materialize the factors and the distinct weights used in them

    # in between the two steps for grounding all factors, weight id range must be decided serially
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors | keys[]
            | "process/grounding/\(.)/materialize_weights"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # partition the id range for weights
        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.inference.factors | keys | map(
                ltrimstr("factor/") | "dd_weights_\(.)" | @sh) | join(" ")) | {
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=${table#dd_weights_}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }

        # set up a union view for all weight tables (dd_graph_weights)
        deepdive sql \("DROP TABLE IF EXISTS dd_graph_weights CASCADE;" | @sh) || true
        deepdive sql \("
            DROP VIEW IF EXISTS dd_graph_weights CASCADE;
            CREATE VIEW dd_graph_weights AS \($deepdive.inference.factors | to_entries | map(
                    (.key | ltrimstr("factor/")) as $factorName | .value
                    | ( [ ("\($factorName)-" | asSqlLiteral)
                        , (.weight_.params[] | "CASE WHEN \(asSqlIdent) IS NULL THEN '\'''\'' ELSE CAST(\(asSqlIdent) AS TEXT) END")
                        ] | join(" ||\("-" | asSqlLiteral)|| ") | "\(.) AS description") as $weightDescExpr
                    | "(SELECT id, isfixed, initvalue, \($weightDescExpr) FROM dd_weights_\($factorName))"
                    # TODO cardinality column for multinomial
                ) | join(" UNION ALL ")
            );" | @sh)
        "
    },

    # each inference rule gets weight ids actually assigned and the factors and weights are dumped into tsv (below)

    # at the very end, everything grounded must be laid down in a format the sampler can load from
    "process/grounding/combine": {
        dependencies_: [(
            $deepdive.schema.variables | to_entries[]
            | .key as $relationName | .value | keys[] | . as $columnName
            | "process/grounding/variable/\($relationName)/dump"
        ), (
            $deepdive.inference.factors | keys[] | ltrimstr("factor/")
            | "process/grounding/factor/\(.)/dump"
        )],
        output_: "model/factorgraph.new",
        style: "cmd_extractor", cmd: "
        error Not implemented

        # TODO generate meta
        # TODO concatenate every binaries dumped so far
        # a la: tobinary.py variables/ factors/ weights/ meta
        "
    }

}

# for each variable add some processes for grounding
| .deepdive_.extraction.extractors += merge($deepdive.schema.variables | to_entries[]
    | .key                   as $relationName
    | .value | to_entries[]
        | .key               as $columnName
        | .value             as $varType
        | "\($relationName)" as $varName
        # TODO handle $varType == Multinomial specially
        | {

            # a process for assigning id to every variable according to the partition
            "process/grounding/variable/\($varName)/assign_id": {
                dependencies_: [
                    "process/grounding/variable_id_partition"
                ],
                style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                table=\($relationName | @sh)
                #column=\($columnName | @sh)

                cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                baseId=$(cat id_begin)

                # assign id to all rows according to the paritition
                # TODO parameterize id column?
                db-assign_sequential_id $table id $baseId
                "
            },

            # a process for dumping each variable table
            "process/grounding/variable/\($varName)/dump": {
                dependencies_: [
                    "process/grounding/variable/\($varName)/assign_id"
                ],
                style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                table=\($relationName | @sh)
                #column=\($columnName | @sh)

                error Not implemented

                # TODO dump the variable table as tsv
                #   joining the holdout query to determine type of each row

                # TODO binarize variable
                # to model/grounding/variable/\($varName).bin,
                "
            }

            # TODO cardinality setup for multinomial

        })


# for each inference rule, add some processes for grounding the factors and weights
| .deepdive_.extraction.extractors += merge($deepdive.inference.factors | to_entries[]
    | (.key | ltrimstr("factor/")) as $factorName | .value
    | {
        # add a process for grounding factors
        "process/grounding/factor/\($factorName)/materialize_weights": {
            # materializing each factor requires the dependent variables to have their id assigned
            dependencies_: [
                .input_[]
                | ltrimstr("data/") as $relationName
                | $deepdive.schema.variables[$relationName] | keys? | .[]
                | . as $columnName
                | "process/grounding/variable/\($relationName)/assign_id"
            ],
            # other non-variable tables are also necessary
            input_: [ .input_[]
                | select(ltrimstr("data/") | $deepdive.schema.variables[.] | not)
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

                # materialize user input_query for the factor
                deepdive sql \("dd_query_\($factorName)" as $factorTable | "
                    DROP TABLE IF EXISTS \($factorTable) CASCADE;
                    CREATE TABLE \($factorTable) AS
                        \(.input_query);
                " | @sh)

                # find distinct weights for the factor
                \(("dd_weights_\($factorName)" | asSqlIdent) as $weightsTable | "
                deepdive sql \("DROP TABLE IF EXISTS \($weightsTable) CASCADE;" | @sh) || true
                deepdive sql \((if .weight_.is_fixed then [] else .weight_.params | map(asSqlIdent) end) as $paramsAsSqlIdents | "
                    DROP VIEW IF EXISTS \($weightsTable) CASCADE;
                    CREATE TABLE \($weightsTable) AS
                        SELECT \(
                            [ $paramsAsSqlIdents[]
                            , "\(.weight_.is_fixed  ) AS isfixed"
                            , "\(.weight_.init_value) AS initvalue"
                            # TODO cast to BIGINT
                            , "-1                     AS id"
                            ] | join(", "))
                         \(if $paramsAsSqlIdents | length == 0 then ""
                         else "FROM dd_query_\($factorName)
                              GROUP BY \($paramsAsSqlIdents | join(", "))"
                         end);
                " | @sh)
                ")
            "
        },

        # add a process for grounding weights per inference rule
        "process/grounding/factor/\($factorName)/assign_weight_id": {
            dependencies_: [
                "process/grounding/weight_id_partition"
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

                cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\($factorName | @sh)
                baseId=$(cat weights_id_begin)

                # assign weight ids according to the partition
                db-assign_sequential_id \("dd_weights_\($factorName)" | @sh) id $baseId
            "
        },

        # add a process for grounding factors and weights
        "process/grounding/factor/\($factorName)/dump": {
            dependencies_: [
                "process/grounding/factor/\($factorName)/assign_weight_id"
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                error Not implemented

                # TODO dump the factors joining the assigned weight ids
                # TODO binarize factors

                # TODO dump the weights (except the description column)
                # TODO binarize weights
            "
        }

    })

' "$@"
