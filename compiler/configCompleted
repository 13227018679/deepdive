#!/usr/bin/env bash
# configCompleted -- Extends normalized deepdive.conf further to include inference data flow
##
exec -a "$0" jq '.deepdive_ as $deepdive

# define the statistical inference processes
| .deepdive_.extraction.extractors += {

        # grounding the factor graph
        "process/model/grounding": {
            dependencies_: ($deepdive.inference.factors | keys),
            output_: "model/factorgraph",
            style: "cmd_extractor",
            cmd: "mkdir -p ../../../model && cd ../../../model
                mkdir -p factorgraph

                # TODO run factor queries and grounding queries directly from here or via database drivers
                set +x; . load-db-driver.sh; set -x
                export DEEPDIVE_LOGFILE=factorgraph/grounding.log
                [[ ! -e \"$DEEPDIVE_LOGFILE\" ]] || mv -f \"$DEEPDIVE_LOGFILE\" \"$DEEPDIVE_LOGFILE\"~
                java org.deepdive.Main -c <(
                    set +x
                    echo \("deepdive \(.deepdive | @json)" | @sh)
                    echo \("deepdive.pipeline.pipelines.grounding: [\(.deepdive.inference.factors | keys | join(", "))]" | @sh)
                    echo \("deepdive.pipeline.run: grounding" | @sh)
                ) -o factorgraph -t inference_grounding

                # drop graph. prefix from file names
                cd factorgraph
                mv -f graph.variables variables
                mv -f graph.factors   factors
                mv -f graph.edges     edges
                mv -f graph.weights   weights
                mv -f graph.meta      meta
            "
        },

        # learning weights and doing inference (since we had to load the graph anyway)
        "process/model/learning": {
            dependencies_: ["model/factorgraph"],
            output_: "model/weights",
            style: "cmd_extractor",
            cmd: "mkdir -p ../../../model && cd ../../../model
                mkdir -p weights
                [ -d factorgraph ] || error \"No factorgraph found\"
                # run inference engine for learning and inference
                \($deepdive.sampler.sampler_cmd // "sampler-dw gibbs") \\
                    -w factorgraph/weights \\
                    -v factorgraph/variables \\
                    -f factorgraph/factors \\
                    -e factorgraph/edges \\
                    -m factorgraph/meta \\
                    -o weights \\
                    \($deepdive.sampler.sampler_args // "#")
                mkdir -p probabilities
                mv -f weights/inference_result.out.text probabilities/
            "
        },

        # performing inference
        "process/model/inference": {
            dependencies_: ["model/factorgraph", "model/weights"],
            output_: "model/probabilities",
            style: "cmd_extractor",
            cmd: "mkdir -p ../../../model && cd ../../../model
                [ -d factorgraph ] || error \"No factorgraph found\"
                if [[ factorgraph/weights -nt probabilities/inference_result.out.text ]]; then
                    # no need to run inference unless the weights are fresher
                    # XXX this skipping may cause confusion
                    # run sampler for performing inference with given weights without learning
                    \($deepdive.sampler.sampler_cmd // "sampler-dw gibbs") \\
                        -l 0 \\
                        -w factorgraph/weights \\
                        -v factorgraph/variables \\
                        -f factorgraph/factors \\
                        -e factorgraph/edges \\
                        -m factorgraph/meta \\
                        -o weights \\
                        \($deepdive.sampler.sampler_args // "#")
                    mkdir -p probabilities
                    mv -f weights/inference_result.out.text probabilities/
                fi
            "
        },

        # loading learning/inference results back to database
        "process/model/load_weights": {
            dependencies_: ["model/weights"],
            output_: "data/model/weights",
            style: "cmd_extractor",
            cmd: "mkdir -p ../../../model && cd ../../../model
                # load weights to database
                deepdive sql \("
                    DROP TABLE IF EXISTS dd_inference_result_weights CASCADE;
                    CREATE TABLE dd_inference_result_weights(
                      id bigint primary key,
                      weight double precision);
                " | @sh)
                cat weights/inference_result.out.weights.text |
                tr \(" "|@sh) \("\\t"|@sh) | DEEPDIVE_LOAD_FORMAT=tsv \\
                deepdive load dd_inference_result_weights /dev/stdin

                # create views
                deepdive sql \("
                    CREATE OR REPLACE VIEW dd_inference_result_weights_mapping AS
                    SELECT dd_graph_weights.*, dd_inference_result_weights.weight FROM
                    dd_graph_weights JOIN dd_inference_result_weights ON dd_graph_weights.id = dd_inference_result_weights.id
                    ORDER BY abs(weight) DESC;

                    CREATE OR REPLACE VIEW dd_inference_result_variables_mapped_weights AS
                    SELECT * FROM dd_inference_result_weights_mapping
                    ORDER BY abs(weight) DESC;
                " | @sh)
            "
        },
        "process/model/load_probabilities": {
            dependencies_: ["model/probabilities"],
            output_: "data/model/probabilities",
            style: "cmd_extractor",
            cmd: "mkdir -p ../../../model && cd ../../../model
                # load weights to database
                deepdive sql \("
                    DROP TABLE IF EXISTS dd_inference_result_variables CASCADE;
                    CREATE TABLE dd_inference_result_variables(
                      id bigint,
                      category bigint,
                      expectation double precision);
                " | @sh)
                cat probabilities/inference_result.out.text |
                tr \(" "|@sh) \("\\t"|@sh) | DEEPDIVE_LOAD_FORMAT=tsv \\
                deepdive load dd_inference_result_variables /dev/stdin

                # create a view for each app schema variable
                deepdive sql \(
                    $deepdive.schema.variables | keys | map(
                    . as $relationName | $deepdive.schema.variables[$relationName] | keys[] |
                    . as $columnName | "
                        CREATE OR REPLACE VIEW \($relationName)_\($columnName)_inference AS
                        (SELECT \($relationName).*, mir.category, mir.expectation FROM
                        \($relationName), dd_inference_result_variables mir
                        WHERE \($relationName).id = mir.id
                        ORDER BY mir.expectation DESC);
                    ") | join("\n") | @sh)
            "
        },

        # calibration plots
        "process/model/calibration": {
            dependencies_: ["data/model/probabilities", "process/model/load_probabilities"],
            output_: "model/calibration-plots",
            style: "cmd_extractor",
            cmd: "
                num_buckets=10
                create_calibration_view() {
                    table=$1 column=$2
                    deepdive sql \"
                        DROP VIEW IF EXISTS ${table}_${column}_inference_bucketed CASCADE;
                        CREATE OR REPLACE VIEW ${table}_${column}_inference_bucketed AS
                            SELECT ${column} AS label
                                 , CASE
                                     WHEN expectation = 1 THEN $(($num_buckets - 1))
                                     ELSE FLOOR(expectation * ${num_buckets})
                                   END AS bucket
                            FROM ${table}_${column}_inference
                        ;

                        DROP VIEW IF EXISTS ${table}_${column}_calibration CASCADE;
                        CREATE OR REPLACE VIEW ${table}_${column}_calibration AS
                         SELECT universe.bucket                             AS bucket
                              , universe.count                              AS num_variables
                              , positive.count                              AS num_correct
                              , negative.count                              AS num_incorrect
                              , universe.bucket        / $num_buckets.      AS probability_lo
                              , (universe.bucket +  1) / ${num_buckets}.    AS probability_hi
                              , (universe.bucket + .5) / ${num_buckets}.    AS probability
                              , 1.0 * positive.count / (
                                positive.count + negative.count)            AS accuracy
                              , positive.count + negative.count             AS num_predictions_test
                              , universe.count                              AS num_predictions_whole
                           FROM (
                                 SELECT bucket, COUNT(*) AS count
                                   FROM ${table}_${column}_inference_bucketed
                                  GROUP BY bucket
                                ) universe
                           LEFT JOIN (
                                 SELECT bucket, COUNT(*) AS count
                                   FROM ${table}_${column}_inference_bucketed
                                  WHERE label = true
                                  GROUP BY bucket
                                ) positive ON universe.bucket = positive.bucket
                           LEFT JOIN (
                                 SELECT bucket, COUNT(*) AS count
                                   FROM ${table}_${column}_inference_bucketed
                                  WHERE label = false
                                  GROUP BY bucket
                                ) negative ON universe.bucket = negative.bucket
                          ORDER BY universe.bucket
                        ;
                    \"
                }
                \($deepdive.schema.variables | keys | map(
                . as $relationName | $deepdive.schema.variables[$relationName] | keys[] |
                . as $columnName | "
                create_calibration_view \($relationName | @sh) \($columnName | @sh)
                ") | join("\n"))
            "
        },

        # TODO processes for incremental workflow: process/model/incremental/materialization, process/model/incremental/learning, process/model/incremental/inference


    }

'
